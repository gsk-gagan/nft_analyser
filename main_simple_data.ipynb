{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import Sequence, Union, Optional, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from scipy.stats import reciprocal\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nft_analyser.transformers import *\n",
    "from nft_analyser import helper\n",
    "\n",
    "# Downgrade to sklearn==0.21.2 for RandomizedSearchCV with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gskgagan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gskgagan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gskgagan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    'glove_features': 300,\n",
    "    'nft_value_range': [0.1, 1e4],      # 10c to $10k\n",
    "    'value_aggregation': ['mean'],\n",
    "    'include_nft_age': True,            # Takes long time (10% out of sample improvement)\n",
    "    'drop_na_age': False,\n",
    "    'learning_rate': 0.8,\n",
    "    'epochs': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector representation of NFT names using pretrained Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NFT Names to Vectors\n",
    "nft_df = helper.get_table(\"nfts\").set_index('address')      # Multiple calls ok as cached at helper level\n",
    "glove_df = helper.get_glove(features=default_params['glove_features'])\n",
    "\n",
    "nft_vec_pp: Pipeline = Pipeline([\n",
    "    ('selectColumns', SelectColumns(columns='name')),\n",
    "    ('onlyFirstCapital', CamelCaseFirstCapital()),\n",
    "    ('camelToWords', CamelCaseToWords()),\n",
    "    ('cleanText', CleanText(regex=r'[^a-zA-Z0-9\\$]')),\n",
    "    ('tokenize', Tokenize()),\n",
    "    ('removeStopWords', RemoveStopWords(nltk.corpus.stopwords.words('english'))),\n",
    "    ('lemmatize', Lemmatize(lemmatizer=nltk.WordNetLemmatizer())),\n",
    "    ('explodeList', ExplodeList()),\n",
    "    ('gloveFeatures', Vectorize(column='name', vectorization_df=glove_df, ignore_missing=True))\n",
    "])\n",
    "\n",
    "nft_vec_df = nft_vec_pp.fit_transform(nft_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking NFT vector to Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction Data\n",
    "trans_df = helper.get_table('transfers')\n",
    "df = trans_df[['nft_address', 'transaction_value']]\n",
    "df['transaction_value'] = df.transaction_value * 3e3 / 1e18   # To USD\n",
    "df = df[(df.transaction_value > default_params['nft_value_range'][0]) & \n",
    "        (df.transaction_value < default_params['nft_value_range'][1])]\n",
    "transaction_df = df.groupby('nft_address').agg({'transaction_value': default_params['value_aggregation']})\n",
    "transaction_df.columns = [c[1] for c in transaction_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Data - NFT vectors to transaction values\n",
    "analysis_df = transaction_df.join(nft_vec_df, how='inner')\n",
    "if default_params['include_nft_age']:\n",
    "    time_df = helper.get_table('mints')\n",
    "    time_df = time_df[['nft_address', 'timestamp']]\n",
    "    time_df = time_df.groupby('nft_address').min()\n",
    "    time_df = (time.time() - time_df) / (3600*24)\n",
    "    analysis_df = analysis_df.join(time_df)\n",
    "    if default_params['drop_na_age']:\n",
    "        analysis_df = analysis_df.dropna()\n",
    "    else:\n",
    "        analysis_df = analysis_df.fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Definition\n",
    "def hyper_neural_network(input_shape:int, output_shape:int, num_layers:int, num_neurons:int, connect_input:bool, \n",
    "                         loss_fn:str, learning_rate:float) -> keras.Model:\n",
    "    input_ = keras.layers.Input(shape=(input_shape,))\n",
    "    last_ = input_\n",
    "    for _ in range(num_layers):\n",
    "        last_ = keras.layers.Dense(num_neurons, activation='relu')(last_)\n",
    "    if connect_input:\n",
    "        last_ = keras.layers.Concatenate()([input_, last_])\n",
    "    output_ = keras.layers.Dense(output_shape, activation='relu')(last_)    # Positive $ values only\n",
    "    \n",
    "    model = keras.Model(inputs=[input_], outputs=[output_])\n",
    "    model.compile(loss=loss_fn, optimizer=keras.optimizers.Adam(lr=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = default_params['value_aggregation']\n",
    "X, y = analysis_df[[c for c in analysis_df.columns if c not in y_cols]], analysis_df[y_cols]\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "train_t, val_t, test_t = (X_train, y_train), (X_valid, y_valid), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gskgagan/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/model_selection/_split.py:442: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "/Users/gskgagan/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/model_selection/_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/Users/gskgagan/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/model_selection/_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/Users/gskgagan/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/model_selection/_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "2022-01-18 23:16:45.329340: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-18 23:16:45.330159: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 10. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 401us/step - loss: 3067264.7000 - val_loss: 2920532.8653\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 161us/step - loss: 2560653.4533 - val_loss: 2856606.8545\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 156us/step - loss: 2432346.2879 - val_loss: 2666685.3220\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 154us/step - loss: 2362072.0629 - val_loss: 2648698.2446\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 147us/step - loss: 2299581.8849 - val_loss: 2659703.2067\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 172us/step - loss: 2226484.0717 - val_loss: 2638031.2601\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 146us/step - loss: 2184196.7304 - val_loss: 2666690.2570\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 153us/step - loss: 2097503.2056 - val_loss: 2676677.6200\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 155us/step - loss: 2034561.3031 - val_loss: 2788878.5426\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 155us/step - loss: 1981209.1322 - val_loss: 2753870.2879\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 1s 306us/step - loss: 1915359.8061 - val_loss: 2785650.3111\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 1s 278us/step - loss: 1858441.8682 - val_loss: 2811580.1184\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 159us/step - loss: 1838921.7922 - val_loss: 2849283.0221\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 1s 205us/step - loss: 1765078.9979 - val_loss: 2920740.2686\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 174us/step - loss: 1771695.3638 - val_loss: 2858176.9598\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 150us/step - loss: 1735440.9540 - val_loss: 2992382.9033\n",
      "1292/1292 [==============================] - 0s 54us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 359us/step - loss: 2990061.1858 - val_loss: 2866184.8638\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 151us/step - loss: 2503393.4372 - val_loss: 2659477.4930\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 160us/step - loss: 2341340.3315 - val_loss: 2618299.1602\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 154us/step - loss: 2270306.3753 - val_loss: 2598653.3444\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 149us/step - loss: 2175450.3588 - val_loss: 2640424.4621\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 157us/step - loss: 2107393.5305 - val_loss: 2664660.0797\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 148us/step - loss: 2035787.0343 - val_loss: 2677703.9203\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 144us/step - loss: 1969437.5529 - val_loss: 2781801.5735\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 152us/step - loss: 1901387.8958 - val_loss: 2814050.1138\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 145us/step - loss: 1844429.1362 - val_loss: 2859176.0557\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 161us/step - loss: 1816838.2284 - val_loss: 2806642.1254\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 153us/step - loss: 1792617.6844 - val_loss: 3079090.4025\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 153us/step - loss: 1759402.6998 - val_loss: 2876242.6649\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 147us/step - loss: 1708268.2220 - val_loss: 2868034.3599\n",
      "1292/1292 [==============================] - 0s 52us/step\n",
      "Train on 2584 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2584/2584 [==============================] - 1s 366us/step - loss: 3182879.5015 - val_loss: 2964106.7423\n",
      "Epoch 2/100\n",
      "2584/2584 [==============================] - 0s 153us/step - loss: 2661581.8853 - val_loss: 2677808.4969\n",
      "Epoch 3/100\n",
      "2584/2584 [==============================] - 0s 147us/step - loss: 2469858.1784 - val_loss: 2621985.0364\n",
      "Epoch 4/100\n",
      "2584/2584 [==============================] - 0s 169us/step - loss: 2390218.1269 - val_loss: 2601158.7500\n",
      "Epoch 5/100\n",
      "2584/2584 [==============================] - 0s 145us/step - loss: 2314447.2844 - val_loss: 2654706.8769\n",
      "Epoch 6/100\n",
      "2584/2584 [==============================] - 0s 154us/step - loss: 2249800.0735 - val_loss: 2634189.1354\n",
      "Epoch 7/100\n",
      "2584/2584 [==============================] - 0s 147us/step - loss: 2180030.8924 - val_loss: 2673314.3429\n",
      "Epoch 8/100\n",
      "2584/2584 [==============================] - 0s 149us/step - loss: 2111664.1683 - val_loss: 2823447.4017\n",
      "Epoch 9/100\n",
      "2584/2584 [==============================] - 1s 195us/step - loss: 2055458.3692 - val_loss: 2908023.2554\n",
      "Epoch 10/100\n",
      "2584/2584 [==============================] - 0s 158us/step - loss: 1981014.3545 - val_loss: 2944220.4923\n",
      "Epoch 11/100\n",
      "2584/2584 [==============================] - 0s 174us/step - loss: 1881462.4605 - val_loss: 3092057.2121\n",
      "Epoch 12/100\n",
      "2584/2584 [==============================] - 0s 151us/step - loss: 1830081.5215 - val_loss: 3085257.6579\n",
      "Epoch 13/100\n",
      "2584/2584 [==============================] - 0s 144us/step - loss: 1769701.9747 - val_loss: 3311388.1223\n",
      "Epoch 14/100\n",
      "2584/2584 [==============================] - 0s 163us/step - loss: 1738917.9276 - val_loss: 3261518.9628\n",
      "1291/1291 [==============================] - 0s 51us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 227us/step - loss: 3531962.5310 - val_loss: 3782860.0372\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 118us/step - loss: 3496088.4297 - val_loss: 3740426.8978\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 3452172.4537 - val_loss: 3690851.2895\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 92us/step - loss: 3399204.7096 - val_loss: 3629112.8870\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 3334777.4773 - val_loss: 3554553.0108\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 98us/step - loss: 3264096.3807 - val_loss: 3478543.4087\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 3190963.5588 - val_loss: 3393804.4892\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 3107516.8840 - val_loss: 3304031.2322\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 3027703.5102 - val_loss: 3224899.7260\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2963497.0221 - val_loss: 3160105.7005\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 103us/step - loss: 2912689.5095 - val_loss: 3111936.8181\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 125us/step - loss: 2876426.0143 - val_loss: 3076683.9427\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2851437.6489 - val_loss: 3054282.4992\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2834667.4012 - val_loss: 3037243.3738\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 93us/step - loss: 2822878.7016 - val_loss: 3025159.8150\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2813664.5823 - val_loss: 3016622.9218\n",
      "Epoch 17/100\n",
      "2583/2583 [==============================] - 0s 107us/step - loss: 2806363.4202 - val_loss: 3009039.1571\n",
      "Epoch 18/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 2799477.1827 - val_loss: 3002943.4002\n",
      "Epoch 19/100\n",
      "2583/2583 [==============================] - 0s 123us/step - loss: 2793246.3076 - val_loss: 2996284.5650\n",
      "Epoch 20/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 2787166.9104 - val_loss: 2990731.5550\n",
      "Epoch 21/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2781004.1302 - val_loss: 2985382.2415\n",
      "Epoch 22/100\n",
      "2583/2583 [==============================] - 0s 91us/step - loss: 2774850.8952 - val_loss: 2980033.9481\n",
      "Epoch 23/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2768712.7637 - val_loss: 2974954.9334\n",
      "Epoch 24/100\n",
      "2583/2583 [==============================] - 0s 92us/step - loss: 2762607.0250 - val_loss: 2970007.1246\n",
      "Epoch 25/100\n",
      "2583/2583 [==============================] - 0s 92us/step - loss: 2756665.9639 - val_loss: 2964479.4683\n",
      "Epoch 26/100\n",
      "2583/2583 [==============================] - 0s 92us/step - loss: 2750287.6898 - val_loss: 2959578.7345\n",
      "Epoch 27/100\n",
      "2583/2583 [==============================] - 0s 93us/step - loss: 2743815.5509 - val_loss: 2954386.4420\n",
      "Epoch 28/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2737646.4356 - val_loss: 2948690.8034\n",
      "Epoch 29/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2731065.7040 - val_loss: 2943573.2895\n",
      "Epoch 30/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2724695.9033 - val_loss: 2938529.1494\n",
      "Epoch 31/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2718299.8213 - val_loss: 2932770.1517\n",
      "Epoch 32/100\n",
      "2583/2583 [==============================] - 0s 94us/step - loss: 2711788.6339 - val_loss: 2927690.5441\n",
      "Epoch 33/100\n",
      "2583/2583 [==============================] - 0s 91us/step - loss: 2705253.5064 - val_loss: 2922712.8971\n",
      "Epoch 34/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2699110.0528 - val_loss: 2917332.6184\n",
      "Epoch 35/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2692626.5621 - val_loss: 2912606.8065\n",
      "Epoch 36/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2686250.4912 - val_loss: 2907061.0317\n",
      "Epoch 37/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2679925.9479 - val_loss: 2902440.1122\n",
      "Epoch 38/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2673825.1440 - val_loss: 2897515.9845\n",
      "Epoch 39/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2668128.9761 - val_loss: 2892364.2252\n",
      "Epoch 40/100\n",
      "2583/2583 [==============================] - 0s 93us/step - loss: 2661545.7827 - val_loss: 2887741.2732\n",
      "Epoch 41/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2655662.7826 - val_loss: 2882650.1463\n",
      "Epoch 42/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2649625.4264 - val_loss: 2877819.5356\n",
      "Epoch 43/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2643712.5599 - val_loss: 2873667.7337\n",
      "Epoch 44/100\n",
      "2583/2583 [==============================] - 0s 101us/step - loss: 2637939.0701 - val_loss: 2869484.5828\n",
      "Epoch 45/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2632400.9981 - val_loss: 2865144.4303\n",
      "Epoch 46/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2627081.0548 - val_loss: 2860281.4706\n",
      "Epoch 47/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2621275.4257 - val_loss: 2856480.7252\n",
      "Epoch 48/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2615822.1483 - val_loss: 2852290.6858\n",
      "Epoch 49/100\n",
      "2583/2583 [==============================] - 0s 103us/step - loss: 2610639.9981 - val_loss: 2848415.8924\n",
      "Epoch 50/100\n",
      "2583/2583 [==============================] - 0s 94us/step - loss: 2605503.5489 - val_loss: 2844670.6099\n",
      "Epoch 51/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2600025.6394 - val_loss: 2840383.5039\n",
      "Epoch 52/100\n",
      "2583/2583 [==============================] - 0s 94us/step - loss: 2595008.6674 - val_loss: 2836899.5132\n",
      "Epoch 53/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2590388.6676 - val_loss: 2832981.8854\n",
      "Epoch 54/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2585153.3949 - val_loss: 2829640.4373\n",
      "Epoch 55/100\n",
      "2583/2583 [==============================] - 0s 98us/step - loss: 2580260.7954 - val_loss: 2826077.4652\n",
      "Epoch 56/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2575731.2500 - val_loss: 2822628.8166\n",
      "Epoch 57/100\n",
      "2583/2583 [==============================] - 0s 103us/step - loss: 2571192.8034 - val_loss: 2819410.4659\n",
      "Epoch 58/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2566613.8201 - val_loss: 2816387.3050\n",
      "Epoch 59/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2562260.8724 - val_loss: 2813185.0658\n",
      "Epoch 60/100\n",
      "2583/2583 [==============================] - 0s 101us/step - loss: 2558001.9457 - val_loss: 2810316.8607\n",
      "Epoch 61/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2553975.1141 - val_loss: 2807357.3994\n",
      "Epoch 62/100\n",
      "2583/2583 [==============================] - 0s 98us/step - loss: 2549919.5336 - val_loss: 2804349.4567\n",
      "Epoch 63/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2546044.1200 - val_loss: 2801858.6842\n",
      "Epoch 64/100\n",
      "2583/2583 [==============================] - 0s 98us/step - loss: 2542091.9368 - val_loss: 2798804.1269\n",
      "Epoch 65/100\n",
      "2583/2583 [==============================] - 0s 104us/step - loss: 2538254.2408 - val_loss: 2796877.0759\n",
      "Epoch 66/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2534594.2416 - val_loss: 2794139.5991\n",
      "Epoch 67/100\n",
      "2583/2583 [==============================] - 0s 94us/step - loss: 2531057.6177 - val_loss: 2791980.3916\n",
      "Epoch 68/100\n",
      "2583/2583 [==============================] - 0s 93us/step - loss: 2527384.5831 - val_loss: 2789643.9822\n",
      "Epoch 69/100\n",
      "2583/2583 [==============================] - 0s 93us/step - loss: 2524190.6130 - val_loss: 2787437.5224\n",
      "Epoch 70/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2520939.2045 - val_loss: 2784915.9257\n",
      "Epoch 71/100\n",
      "2583/2583 [==============================] - 0s 93us/step - loss: 2517579.6226 - val_loss: 2783354.7864\n",
      "Epoch 72/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2514418.3130 - val_loss: 2781432.6502\n",
      "Epoch 73/100\n",
      "2583/2583 [==============================] - 0s 94us/step - loss: 2511521.5402 - val_loss: 2779138.5759\n",
      "Epoch 74/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2508706.4816 - val_loss: 2777391.7028\n",
      "Epoch 75/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2505514.4613 - val_loss: 2775646.0008\n",
      "Epoch 76/100\n",
      "2583/2583 [==============================] - 0s 119us/step - loss: 2502479.6015 - val_loss: 2774181.4118\n",
      "Epoch 77/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2499736.6336 - val_loss: 2772182.9211\n",
      "Epoch 78/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2496864.4223 - val_loss: 2770748.1246\n",
      "Epoch 79/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2494698.6674 - val_loss: 2769322.2291\n",
      "Epoch 80/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2491728.8178 - val_loss: 2768304.7283\n",
      "Epoch 81/100\n",
      "2583/2583 [==============================] - 0s 98us/step - loss: 2489401.1702 - val_loss: 2766615.5139\n",
      "Epoch 82/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2486863.8355 - val_loss: 2764693.4791\n",
      "Epoch 83/100\n",
      "2583/2583 [==============================] - 0s 95us/step - loss: 2484493.7964 - val_loss: 2763699.8560\n",
      "Epoch 84/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2481484.5217 - val_loss: 2762002.8026\n",
      "Epoch 85/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2479215.2182 - val_loss: 2760592.1788\n",
      "Epoch 86/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2477096.7113 - val_loss: 2759322.5457\n",
      "Epoch 87/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2474998.2917 - val_loss: 2758913.9559\n",
      "Epoch 88/100\n",
      "2583/2583 [==============================] - 1s 212us/step - loss: 2472708.0324 - val_loss: 2757259.7098\n",
      "Epoch 89/100\n",
      "2583/2583 [==============================] - 1s 247us/step - loss: 2470767.4116 - val_loss: 2755880.6200\n",
      "Epoch 90/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2468073.8628 - val_loss: 2755138.5511\n",
      "Epoch 91/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2465721.5227 - val_loss: 2753514.0673\n",
      "Epoch 92/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2463922.0959 - val_loss: 2752630.5031\n",
      "Epoch 93/100\n",
      "2583/2583 [==============================] - 0s 94us/step - loss: 2461981.9272 - val_loss: 2751971.8808\n",
      "Epoch 94/100\n",
      "2583/2583 [==============================] - 0s 105us/step - loss: 2459757.3660 - val_loss: 2750764.7577\n",
      "Epoch 95/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2457824.5937 - val_loss: 2750235.7067\n",
      "Epoch 96/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 2455967.8689 - val_loss: 2749156.8568\n",
      "Epoch 97/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2454191.7650 - val_loss: 2747778.8460\n",
      "Epoch 98/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2452130.9656 - val_loss: 2746827.8088\n",
      "Epoch 99/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2450196.6652 - val_loss: 2746065.5039\n",
      "Epoch 100/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2448513.9022 - val_loss: 2745213.2988\n",
      "1292/1292 [==============================] - 0s 32us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 234us/step - loss: 3527438.5577 - val_loss: 3754709.9102\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 121us/step - loss: 3479996.8221 - val_loss: 3703267.6610\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 3427972.6723 - val_loss: 3645733.4582\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 3367611.3848 - val_loss: 3575892.5031\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 101us/step - loss: 3295505.0256 - val_loss: 3498632.3560\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 3221708.8623 - val_loss: 3420744.7384\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 3149290.1006 - val_loss: 3347367.6347\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 3081511.0228 - val_loss: 3279997.6656\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 3020462.2163 - val_loss: 3219028.1037\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 108us/step - loss: 2967813.9919 - val_loss: 3166746.6788\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2921673.6293 - val_loss: 3120253.4009\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 107us/step - loss: 2881491.8050 - val_loss: 3080872.5163\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 108us/step - loss: 2853299.4785 - val_loss: 3056700.8816\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2836035.8810 - val_loss: 3041395.6416\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2825060.0351 - val_loss: 3031647.5116\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2817174.4482 - val_loss: 3024089.9598\n",
      "Epoch 17/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2810836.3579 - val_loss: 3019015.9915\n",
      "Epoch 18/100\n",
      "2583/2583 [==============================] - 0s 103us/step - loss: 2805465.1715 - val_loss: 3013617.0255\n",
      "Epoch 19/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2800248.4887 - val_loss: 3008766.8460\n",
      "Epoch 20/100\n",
      "2583/2583 [==============================] - 0s 107us/step - loss: 2795232.2548 - val_loss: 3004288.9474\n",
      "Epoch 21/100\n",
      "2583/2583 [==============================] - 0s 103us/step - loss: 2790272.0997 - val_loss: 3000040.2554\n",
      "Epoch 22/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 2785648.5261 - val_loss: 2995472.4698\n",
      "Epoch 23/100\n",
      "2583/2583 [==============================] - 0s 105us/step - loss: 2780610.2394 - val_loss: 2991439.1703\n",
      "Epoch 24/100\n",
      "2583/2583 [==============================] - 0s 105us/step - loss: 2775847.9674 - val_loss: 2987365.6246\n",
      "Epoch 25/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2771042.1098 - val_loss: 2983003.8050\n",
      "Epoch 26/100\n",
      "2583/2583 [==============================] - 0s 103us/step - loss: 2766006.3464 - val_loss: 2978830.5519\n",
      "Epoch 27/100\n",
      "2583/2583 [==============================] - 0s 104us/step - loss: 2760987.5565 - val_loss: 2975091.8955\n",
      "Epoch 28/100\n",
      "2583/2583 [==============================] - 0s 108us/step - loss: 2756035.9007 - val_loss: 2970966.1053\n",
      "Epoch 29/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 2751037.7496 - val_loss: 2966703.2902\n",
      "Epoch 30/100\n",
      "2583/2583 [==============================] - 0s 105us/step - loss: 2746108.4381 - val_loss: 2962523.4172\n",
      "Epoch 31/100\n",
      "2583/2583 [==============================] - 0s 105us/step - loss: 2741331.8858 - val_loss: 2958349.1966\n",
      "Epoch 32/100\n",
      "2583/2583 [==============================] - 0s 112us/step - loss: 2736003.0347 - val_loss: 2953913.1904\n",
      "Epoch 33/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2730777.9033 - val_loss: 2950028.4319\n",
      "Epoch 34/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2726022.4372 - val_loss: 2945821.5658\n",
      "Epoch 35/100\n",
      "2583/2583 [==============================] - 0s 182us/step - loss: 2720985.7589 - val_loss: 2941850.1757\n",
      "Epoch 36/100\n",
      "2583/2583 [==============================] - 0s 129us/step - loss: 2715722.6038 - val_loss: 2937501.8166\n",
      "Epoch 37/100\n",
      "2583/2583 [==============================] - 0s 119us/step - loss: 2710650.2932 - val_loss: 2933229.7515\n",
      "Epoch 38/100\n",
      "2583/2583 [==============================] - 0s 122us/step - loss: 2705674.9315 - val_loss: 2928947.2655\n",
      "Epoch 39/100\n",
      "2583/2583 [==============================] - 0s 131us/step - loss: 2700606.8212 - val_loss: 2925201.3413\n",
      "Epoch 40/100\n",
      "2583/2583 [==============================] - 0s 128us/step - loss: 2695466.0128 - val_loss: 2920893.6022\n",
      "Epoch 41/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2690563.3052 - val_loss: 2916951.1486\n",
      "Epoch 42/100\n",
      "2583/2583 [==============================] - 0s 111us/step - loss: 2685358.6966 - val_loss: 2912906.6471\n",
      "Epoch 43/100\n",
      "2583/2583 [==============================] - 0s 121us/step - loss: 2680437.0226 - val_loss: 2908724.9079\n",
      "Epoch 44/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2675511.9144 - val_loss: 2904835.0557\n",
      "Epoch 45/100\n",
      "2583/2583 [==============================] - 0s 127us/step - loss: 2670252.3242 - val_loss: 2900938.3847\n",
      "Epoch 46/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2665387.3548 - val_loss: 2896840.5046\n",
      "Epoch 47/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2660663.7732 - val_loss: 2893019.6726\n",
      "Epoch 48/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2655528.4611 - val_loss: 2889276.9373\n",
      "Epoch 49/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2650846.8022 - val_loss: 2885453.1749\n",
      "Epoch 50/100\n",
      "2583/2583 [==============================] - 0s 101us/step - loss: 2646233.5941 - val_loss: 2881730.8367\n",
      "Epoch 51/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2641351.1571 - val_loss: 2878146.0635\n",
      "Epoch 52/100\n",
      "2583/2583 [==============================] - 0s 101us/step - loss: 2636844.3703 - val_loss: 2874187.8545\n",
      "Epoch 53/100\n",
      "2583/2583 [==============================] - 0s 101us/step - loss: 2632121.0736 - val_loss: 2870826.8816\n",
      "Epoch 54/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2627410.2920 - val_loss: 2867184.6734\n",
      "Epoch 55/100\n",
      "2583/2583 [==============================] - 0s 101us/step - loss: 2622979.3494 - val_loss: 2863952.6780\n",
      "Epoch 56/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2618448.3864 - val_loss: 2860317.6726\n",
      "Epoch 57/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2614252.5652 - val_loss: 2856808.4868\n",
      "Epoch 58/100\n",
      "2583/2583 [==============================] - 0s 98us/step - loss: 2609608.4829 - val_loss: 2853710.1502\n",
      "Epoch 59/100\n",
      "2583/2583 [==============================] - 0s 108us/step - loss: 2605634.5186 - val_loss: 2850586.2918\n",
      "Epoch 60/100\n",
      "2583/2583 [==============================] - 0s 111us/step - loss: 2600975.5384 - val_loss: 2847409.3576\n",
      "Epoch 61/100\n",
      "2583/2583 [==============================] - 0s 125us/step - loss: 2596696.2803 - val_loss: 2844359.0372\n",
      "Epoch 62/100\n",
      "2583/2583 [==============================] - 0s 125us/step - loss: 2592629.6811 - val_loss: 2841042.1153\n",
      "Epoch 63/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2588713.4424 - val_loss: 2838007.6231\n",
      "Epoch 64/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2584550.2892 - val_loss: 2835183.3406\n",
      "Epoch 65/100\n",
      "2583/2583 [==============================] - 0s 117us/step - loss: 2580324.1252 - val_loss: 2832407.2237\n",
      "Epoch 66/100\n",
      "2583/2583 [==============================] - 0s 129us/step - loss: 2576553.3680 - val_loss: 2829287.6362\n",
      "Epoch 67/100\n",
      "2583/2583 [==============================] - 2s 831us/step - loss: 2572801.3722 - val_loss: 2826315.8584\n",
      "Epoch 68/100\n",
      "2583/2583 [==============================] - 0s 167us/step - loss: 2569032.4290 - val_loss: 2823806.9884\n",
      "Epoch 69/100\n",
      "2583/2583 [==============================] - 0s 148us/step - loss: 2565234.5689 - val_loss: 2821440.1695\n",
      "Epoch 70/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2561541.7904 - val_loss: 2818642.1796\n",
      "Epoch 71/100\n",
      "2583/2583 [==============================] - 0s 101us/step - loss: 2557614.8031 - val_loss: 2816172.7067\n",
      "Epoch 72/100\n",
      "2583/2583 [==============================] - 0s 104us/step - loss: 2554096.0407 - val_loss: 2813607.4729\n",
      "Epoch 73/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 2550725.4844 - val_loss: 2811057.3150\n",
      "Epoch 74/100\n",
      "2583/2583 [==============================] - 0s 118us/step - loss: 2547037.2236 - val_loss: 2808761.9729\n",
      "Epoch 75/100\n",
      "2583/2583 [==============================] - 0s 105us/step - loss: 2543606.5572 - val_loss: 2806258.1091\n",
      "Epoch 76/100\n",
      "2583/2583 [==============================] - 0s 103us/step - loss: 2540253.8522 - val_loss: 2803983.9992\n",
      "Epoch 77/100\n",
      "2583/2583 [==============================] - 0s 105us/step - loss: 2536868.0411 - val_loss: 2801881.4249\n",
      "Epoch 78/100\n",
      "2583/2583 [==============================] - 0s 101us/step - loss: 2533635.3250 - val_loss: 2799400.7546\n",
      "Epoch 79/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2530206.6250 - val_loss: 2797635.0944\n",
      "Epoch 80/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2527356.2228 - val_loss: 2795660.2283\n",
      "Epoch 81/100\n",
      "2583/2583 [==============================] - 0s 98us/step - loss: 2524358.2695 - val_loss: 2793686.5015\n",
      "Epoch 82/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2520849.9406 - val_loss: 2791571.7717\n",
      "Epoch 83/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2518272.6602 - val_loss: 2789127.9226\n",
      "Epoch 84/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2514832.4674 - val_loss: 2787533.7910\n",
      "Epoch 85/100\n",
      "2583/2583 [==============================] - 0s 98us/step - loss: 2512063.6779 - val_loss: 2785985.7353\n",
      "Epoch 86/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2509241.1549 - val_loss: 2783929.9458\n",
      "Epoch 87/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 2506236.6493 - val_loss: 2782342.1014\n",
      "Epoch 88/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2503530.0153 - val_loss: 2780915.1664\n",
      "Epoch 89/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2500805.8990 - val_loss: 2778694.5766\n",
      "Epoch 90/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2498047.2640 - val_loss: 2777090.3104\n",
      "Epoch 91/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2495555.4756 - val_loss: 2775640.1904\n",
      "Epoch 92/100\n",
      "2583/2583 [==============================] - 0s 111us/step - loss: 2492550.4959 - val_loss: 2773744.9574\n",
      "Epoch 93/100\n",
      "2583/2583 [==============================] - 0s 112us/step - loss: 2489974.3686 - val_loss: 2772403.3615\n",
      "Epoch 94/100\n",
      "2583/2583 [==============================] - 0s 101us/step - loss: 2487835.1642 - val_loss: 2770645.0085\n",
      "Epoch 95/100\n",
      "2583/2583 [==============================] - 0s 122us/step - loss: 2484832.8083 - val_loss: 2769503.7608\n",
      "Epoch 96/100\n",
      "2583/2583 [==============================] - 0s 130us/step - loss: 2482125.1658 - val_loss: 2767604.8173\n",
      "Epoch 97/100\n",
      "2583/2583 [==============================] - 0s 141us/step - loss: 2479690.7319 - val_loss: 2766074.3715\n",
      "Epoch 98/100\n",
      "2583/2583 [==============================] - 0s 117us/step - loss: 2477487.4735 - val_loss: 2764951.5116\n",
      "Epoch 99/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 2475198.3865 - val_loss: 2763203.9543\n",
      "Epoch 100/100\n",
      "2583/2583 [==============================] - 0s 111us/step - loss: 2472362.4978 - val_loss: 2761627.1950\n",
      "1292/1292 [==============================] - 0s 37us/step\n",
      "Train on 2584 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2584/2584 [==============================] - 1s 246us/step - loss: 3599002.8359 - val_loss: 3772409.9752\n",
      "Epoch 2/100\n",
      "2584/2584 [==============================] - 0s 130us/step - loss: 3550901.7392 - val_loss: 3716868.0093\n",
      "Epoch 3/100\n",
      "2584/2584 [==============================] - 0s 123us/step - loss: 3496074.2601 - val_loss: 3654958.9319\n",
      "Epoch 4/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 3434666.5410 - val_loss: 3584240.3762\n",
      "Epoch 5/100\n",
      "2584/2584 [==============================] - 0s 94us/step - loss: 3366143.6633 - val_loss: 3508799.9319\n",
      "Epoch 6/100\n",
      "2584/2584 [==============================] - 0s 95us/step - loss: 3290457.3400 - val_loss: 3421534.8390\n",
      "Epoch 7/100\n",
      "2584/2584 [==============================] - 0s 100us/step - loss: 3205258.3371 - val_loss: 3322739.2074\n",
      "Epoch 8/100\n",
      "2584/2584 [==============================] - 0s 107us/step - loss: 3117167.1171 - val_loss: 3231856.4861\n",
      "Epoch 9/100\n",
      "2584/2584 [==============================] - 0s 100us/step - loss: 3045786.4207 - val_loss: 3165438.4868\n",
      "Epoch 10/100\n",
      "2584/2584 [==============================] - 0s 104us/step - loss: 2994467.3738 - val_loss: 3115190.3584\n",
      "Epoch 11/100\n",
      "2584/2584 [==============================] - 0s 95us/step - loss: 2957854.1947 - val_loss: 3081415.6068\n",
      "Epoch 12/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2933384.0882 - val_loss: 3058403.0751\n",
      "Epoch 13/100\n",
      "2584/2584 [==============================] - 0s 168us/step - loss: 2916969.2752 - val_loss: 3040632.5201\n",
      "Epoch 14/100\n",
      "2584/2584 [==============================] - 2s 892us/step - loss: 2905630.5426 - val_loss: 3030821.3444\n",
      "Epoch 15/100\n",
      "2584/2584 [==============================] - 0s 129us/step - loss: 2897252.4903 - val_loss: 3022579.7152\n",
      "Epoch 16/100\n",
      "2584/2584 [==============================] - 0s 120us/step - loss: 2890507.0917 - val_loss: 3015518.7593\n",
      "Epoch 17/100\n",
      "2584/2584 [==============================] - 1s 224us/step - loss: 2884976.4284 - val_loss: 3009670.4172\n",
      "Epoch 18/100\n",
      "2584/2584 [==============================] - 0s 155us/step - loss: 2879549.4009 - val_loss: 3004934.2477\n",
      "Epoch 19/100\n",
      "2584/2584 [==============================] - 0s 103us/step - loss: 2874094.0499 - val_loss: 2999721.1370\n",
      "Epoch 20/100\n",
      "2584/2584 [==============================] - 0s 97us/step - loss: 2869486.7771 - val_loss: 2994885.0178\n",
      "Epoch 21/100\n",
      "2584/2584 [==============================] - 0s 94us/step - loss: 2863603.0046 - val_loss: 2990260.9033\n",
      "Epoch 22/100\n",
      "2584/2584 [==============================] - 0s 95us/step - loss: 2858234.1718 - val_loss: 2985049.7887\n",
      "Epoch 23/100\n",
      "2584/2584 [==============================] - 0s 95us/step - loss: 2852636.8533 - val_loss: 2980485.4799\n",
      "Epoch 24/100\n",
      "2584/2584 [==============================] - 0s 100us/step - loss: 2847161.0561 - val_loss: 2975230.2361\n",
      "Epoch 25/100\n",
      "2584/2584 [==============================] - 0s 97us/step - loss: 2841580.7136 - val_loss: 2970377.5906\n",
      "Epoch 26/100\n",
      "2584/2584 [==============================] - 0s 106us/step - loss: 2835824.1076 - val_loss: 2965216.0542\n",
      "Epoch 27/100\n",
      "2584/2584 [==============================] - 0s 97us/step - loss: 2830400.4044 - val_loss: 2960298.9745\n",
      "Epoch 28/100\n",
      "2584/2584 [==============================] - 0s 92us/step - loss: 2824551.5089 - val_loss: 2955409.2082\n",
      "Epoch 29/100\n",
      "2584/2584 [==============================] - 0s 108us/step - loss: 2818831.1811 - val_loss: 2950944.8483\n",
      "Epoch 30/100\n",
      "2584/2584 [==============================] - 0s 99us/step - loss: 2813242.3080 - val_loss: 2945860.2376\n",
      "Epoch 31/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2807514.4915 - val_loss: 2941360.0054\n",
      "Epoch 32/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2801947.6885 - val_loss: 2935660.1409\n",
      "Epoch 33/100\n",
      "2584/2584 [==============================] - 0s 128us/step - loss: 2796757.8495 - val_loss: 2931157.9474\n",
      "Epoch 34/100\n",
      "2584/2584 [==============================] - 0s 118us/step - loss: 2790823.1490 - val_loss: 2926416.2647\n",
      "Epoch 35/100\n",
      "2584/2584 [==============================] - 0s 122us/step - loss: 2785235.9385 - val_loss: 2921488.9868\n",
      "Epoch 36/100\n",
      "2584/2584 [==============================] - 0s 114us/step - loss: 2779861.1188 - val_loss: 2916753.9466\n",
      "Epoch 37/100\n",
      "2584/2584 [==============================] - 0s 120us/step - loss: 2774323.0201 - val_loss: 2912925.8382\n",
      "Epoch 38/100\n",
      "2584/2584 [==============================] - 0s 118us/step - loss: 2768933.3235 - val_loss: 2907552.5720\n",
      "Epoch 39/100\n",
      "2584/2584 [==============================] - 0s 131us/step - loss: 2763392.8816 - val_loss: 2903348.8320\n",
      "Epoch 40/100\n",
      "2584/2584 [==============================] - 0s 129us/step - loss: 2758188.8572 - val_loss: 2898480.8104\n",
      "Epoch 41/100\n",
      "2584/2584 [==============================] - 0s 116us/step - loss: 2752960.0143 - val_loss: 2893949.5147\n",
      "Epoch 42/100\n",
      "2584/2584 [==============================] - 0s 125us/step - loss: 2747854.2361 - val_loss: 2890008.2957\n",
      "Epoch 43/100\n",
      "2584/2584 [==============================] - 0s 120us/step - loss: 2742516.8522 - val_loss: 2885260.1463\n",
      "Epoch 44/100\n",
      "2584/2584 [==============================] - 0s 127us/step - loss: 2737264.4218 - val_loss: 2880911.8847\n",
      "Epoch 45/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2732119.2113 - val_loss: 2876580.8553\n",
      "Epoch 46/100\n",
      "2584/2584 [==============================] - 0s 106us/step - loss: 2727134.4671 - val_loss: 2872423.3615\n",
      "Epoch 47/100\n",
      "2584/2584 [==============================] - 0s 128us/step - loss: 2722047.9770 - val_loss: 2868300.3289\n",
      "Epoch 48/100\n",
      "2584/2584 [==============================] - 0s 95us/step - loss: 2717194.5147 - val_loss: 2864530.7283\n",
      "Epoch 49/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2712319.4574 - val_loss: 2859837.7693\n",
      "Epoch 50/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2707444.4640 - val_loss: 2856291.4079\n",
      "Epoch 51/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2702579.3665 - val_loss: 2852430.4358\n",
      "Epoch 52/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2697589.2593 - val_loss: 2847924.8073\n",
      "Epoch 53/100\n",
      "2584/2584 [==============================] - 0s 95us/step - loss: 2693101.3738 - val_loss: 2844541.4868\n",
      "Epoch 54/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2688281.0050 - val_loss: 2840540.6045\n",
      "Epoch 55/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2684104.4191 - val_loss: 2837125.2485\n",
      "Epoch 56/100\n",
      "2584/2584 [==============================] - 0s 95us/step - loss: 2679266.7852 - val_loss: 2833224.5147\n",
      "Epoch 57/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2674882.5530 - val_loss: 2829484.5062\n",
      "Epoch 58/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2670409.9779 - val_loss: 2826056.2624\n",
      "Epoch 59/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2666131.3398 - val_loss: 2822785.5666\n",
      "Epoch 60/100\n",
      "2584/2584 [==============================] - 0s 110us/step - loss: 2661765.2937 - val_loss: 2819113.1587\n",
      "Epoch 61/100\n",
      "2584/2584 [==============================] - 0s 92us/step - loss: 2657714.3893 - val_loss: 2815559.1494\n",
      "Epoch 62/100\n",
      "2584/2584 [==============================] - 0s 92us/step - loss: 2653483.2469 - val_loss: 2812672.0232\n",
      "Epoch 63/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2649781.0967 - val_loss: 2809384.4156\n",
      "Epoch 64/100\n",
      "2584/2584 [==============================] - 0s 92us/step - loss: 2645648.6463 - val_loss: 2806100.3607\n",
      "Epoch 65/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2641710.1358 - val_loss: 2803582.9636\n",
      "Epoch 66/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2637731.7547 - val_loss: 2800161.7841\n",
      "Epoch 67/100\n",
      "2584/2584 [==============================] - 0s 93us/step - loss: 2633839.6502 - val_loss: 2797212.1122\n",
      "Epoch 68/100\n",
      "2584/2584 [==============================] - 0s 92us/step - loss: 2630182.9168 - val_loss: 2794169.8986\n",
      "Epoch 69/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2626449.5513 - val_loss: 2791407.6656\n",
      "Epoch 70/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2623093.6631 - val_loss: 2788860.1594\n",
      "Epoch 71/100\n",
      "2584/2584 [==============================] - 0s 94us/step - loss: 2619489.2376 - val_loss: 2786216.8197\n",
      "Epoch 72/100\n",
      "2584/2584 [==============================] - 0s 101us/step - loss: 2616158.6026 - val_loss: 2783485.9327\n",
      "Epoch 73/100\n",
      "2584/2584 [==============================] - 0s 93us/step - loss: 2612534.8030 - val_loss: 2781243.6440\n",
      "Epoch 74/100\n",
      "2584/2584 [==============================] - 0s 95us/step - loss: 2609318.8425 - val_loss: 2778621.2190\n",
      "Epoch 75/100\n",
      "2584/2584 [==============================] - 0s 93us/step - loss: 2606071.1142 - val_loss: 2776505.0921\n",
      "Epoch 76/100\n",
      "2584/2584 [==============================] - 0s 93us/step - loss: 2602678.0491 - val_loss: 2773640.6741\n",
      "Epoch 77/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2599478.9787 - val_loss: 2771247.8313\n",
      "Epoch 78/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2596447.0859 - val_loss: 2769089.9822\n",
      "Epoch 79/100\n",
      "2584/2584 [==============================] - 0s 115us/step - loss: 2593633.4861 - val_loss: 2767022.7515\n",
      "Epoch 80/100\n",
      "2584/2584 [==============================] - 0s 100us/step - loss: 2590406.2276 - val_loss: 2764618.0201\n",
      "Epoch 81/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2587468.4137 - val_loss: 2762260.5666\n",
      "Epoch 82/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2585088.8634 - val_loss: 2760401.2245\n",
      "Epoch 83/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2581920.5857 - val_loss: 2758301.1339\n",
      "Epoch 84/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2579269.9702 - val_loss: 2756575.5805\n",
      "Epoch 85/100\n",
      "2584/2584 [==============================] - 0s 126us/step - loss: 2576596.9632 - val_loss: 2754903.2949\n",
      "Epoch 86/100\n",
      "2584/2584 [==============================] - 0s 104us/step - loss: 2573987.1103 - val_loss: 2752824.3375\n",
      "Epoch 87/100\n",
      "2584/2584 [==============================] - 0s 111us/step - loss: 2571194.4625 - val_loss: 2750505.9628\n",
      "Epoch 88/100\n",
      "2584/2584 [==============================] - 0s 122us/step - loss: 2568634.4733 - val_loss: 2748937.3971\n",
      "Epoch 89/100\n",
      "2584/2584 [==============================] - 0s 144us/step - loss: 2565941.7995 - val_loss: 2747450.2485\n",
      "Epoch 90/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2563688.4915 - val_loss: 2745273.2090\n",
      "Epoch 91/100\n",
      "2584/2584 [==============================] - 0s 144us/step - loss: 2560906.8885 - val_loss: 2743950.2268\n",
      "Epoch 92/100\n",
      "2584/2584 [==============================] - 0s 102us/step - loss: 2558812.6126 - val_loss: 2742288.4334\n",
      "Epoch 93/100\n",
      "2584/2584 [==============================] - 0s 120us/step - loss: 2556300.2005 - val_loss: 2740492.2392\n",
      "Epoch 94/100\n",
      "2584/2584 [==============================] - 0s 128us/step - loss: 2553779.4126 - val_loss: 2738946.8862\n",
      "Epoch 95/100\n",
      "2584/2584 [==============================] - 0s 123us/step - loss: 2551471.7574 - val_loss: 2737439.3591\n",
      "Epoch 96/100\n",
      "2584/2584 [==============================] - 0s 149us/step - loss: 2549033.1741 - val_loss: 2736066.9404\n",
      "Epoch 97/100\n",
      "2584/2584 [==============================] - 0s 117us/step - loss: 2546719.1493 - val_loss: 2734250.5426\n",
      "Epoch 98/100\n",
      "2584/2584 [==============================] - 0s 117us/step - loss: 2544478.0488 - val_loss: 2732827.3661\n",
      "Epoch 99/100\n",
      "2584/2584 [==============================] - 0s 127us/step - loss: 2542656.6885 - val_loss: 2731784.9296\n",
      "Epoch 100/100\n",
      "2584/2584 [==============================] - 0s 117us/step - loss: 2540172.2779 - val_loss: 2730182.3986\n",
      "1291/1291 [==============================] - 0s 47us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 270us/step - loss: 3459537.5275 - val_loss: 3624246.6749\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 159us/step - loss: 3231374.9411 - val_loss: 3329233.0666\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 154us/step - loss: 2972448.0286 - val_loss: 3104202.3011\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 130us/step - loss: 2844390.8591 - val_loss: 3024920.7856\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2805212.0005 - val_loss: 2994296.0248\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 128us/step - loss: 2776237.6616 - val_loss: 2970056.0488\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 124us/step - loss: 2744581.3524 - val_loss: 2940925.7701\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 130us/step - loss: 2713686.6563 - val_loss: 2913631.3259\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2680378.4863 - val_loss: 2886489.4961\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2648925.6090 - val_loss: 2861769.3909\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2620829.0882 - val_loss: 2841471.0766\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 163us/step - loss: 2593927.4280 - val_loss: 2820466.9683\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2570711.5941 - val_loss: 2805037.2554\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 118us/step - loss: 2550160.1165 - val_loss: 2789880.1122\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2532094.0998 - val_loss: 2778745.6153\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 127us/step - loss: 2517548.2811 - val_loss: 2768469.2624\n",
      "Epoch 17/100\n",
      "2583/2583 [==============================] - 0s 118us/step - loss: 2503476.3567 - val_loss: 2759753.7632\n",
      "Epoch 18/100\n",
      "2583/2583 [==============================] - 0s 140us/step - loss: 2491871.3966 - val_loss: 2753457.9358\n",
      "Epoch 19/100\n",
      "2583/2583 [==============================] - 0s 132us/step - loss: 2482029.5583 - val_loss: 2746210.9969\n",
      "Epoch 20/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2472278.4154 - val_loss: 2741800.0023\n",
      "Epoch 21/100\n",
      "2583/2583 [==============================] - 0s 128us/step - loss: 2464298.0808 - val_loss: 2736860.9551\n",
      "Epoch 22/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2456876.4908 - val_loss: 2731989.3367\n",
      "Epoch 23/100\n",
      "2583/2583 [==============================] - 0s 129us/step - loss: 2449437.9358 - val_loss: 2727971.7577\n",
      "Epoch 24/100\n",
      "2583/2583 [==============================] - 0s 144us/step - loss: 2443646.2970 - val_loss: 2724307.3197\n",
      "Epoch 25/100\n",
      "2583/2583 [==============================] - 0s 114us/step - loss: 2437126.5714 - val_loss: 2721366.8344\n",
      "Epoch 26/100\n",
      "2583/2583 [==============================] - 0s 128us/step - loss: 2431032.7908 - val_loss: 2717996.2252\n",
      "Epoch 27/100\n",
      "2583/2583 [==============================] - 0s 118us/step - loss: 2425220.4111 - val_loss: 2715062.5967\n",
      "Epoch 28/100\n",
      "2583/2583 [==============================] - 0s 145us/step - loss: 2420329.8844 - val_loss: 2713142.5697\n",
      "Epoch 29/100\n",
      "2583/2583 [==============================] - 0s 147us/step - loss: 2415318.3205 - val_loss: 2709650.3460\n",
      "Epoch 30/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2409537.4443 - val_loss: 2705848.0031\n",
      "Epoch 31/100\n",
      "2583/2583 [==============================] - 0s 140us/step - loss: 2404662.1807 - val_loss: 2704600.2701\n",
      "Epoch 32/100\n",
      "2583/2583 [==============================] - 0s 121us/step - loss: 2401019.7472 - val_loss: 2700903.8104\n",
      "Epoch 33/100\n",
      "2583/2583 [==============================] - 0s 154us/step - loss: 2394483.6965 - val_loss: 2700117.9907\n",
      "Epoch 34/100\n",
      "2583/2583 [==============================] - 0s 146us/step - loss: 2389492.5637 - val_loss: 2697181.5797\n",
      "Epoch 35/100\n",
      "2583/2583 [==============================] - 0s 117us/step - loss: 2385993.4839 - val_loss: 2695338.9365\n",
      "Epoch 36/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2379635.7447 - val_loss: 2694298.5410\n",
      "Epoch 37/100\n",
      "2583/2583 [==============================] - 0s 132us/step - loss: 2373733.4952 - val_loss: 2688139.5929\n",
      "Epoch 38/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2370371.9503 - val_loss: 2685893.6509\n",
      "Epoch 39/100\n",
      "2583/2583 [==============================] - 0s 111us/step - loss: 2366087.2515 - val_loss: 2684079.9474\n",
      "Epoch 40/100\n",
      "2583/2583 [==============================] - 0s 111us/step - loss: 2358290.0812 - val_loss: 2683197.0557\n",
      "Epoch 41/100\n",
      "2583/2583 [==============================] - 0s 114us/step - loss: 2353581.8285 - val_loss: 2680183.2802\n",
      "Epoch 42/100\n",
      "2583/2583 [==============================] - 0s 114us/step - loss: 2348778.1300 - val_loss: 2678606.5929\n",
      "Epoch 43/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2343005.4686 - val_loss: 2676628.7291\n",
      "Epoch 44/100\n",
      "2583/2583 [==============================] - 0s 112us/step - loss: 2338683.5822 - val_loss: 2673703.8955\n",
      "Epoch 45/100\n",
      "2583/2583 [==============================] - 0s 119us/step - loss: 2332893.9152 - val_loss: 2672590.7763\n",
      "Epoch 46/100\n",
      "2583/2583 [==============================] - 0s 174us/step - loss: 2327362.1145 - val_loss: 2671817.4234\n",
      "Epoch 47/100\n",
      "2583/2583 [==============================] - 0s 145us/step - loss: 2323147.3222 - val_loss: 2670759.4218\n",
      "Epoch 48/100\n",
      "2583/2583 [==============================] - 0s 147us/step - loss: 2317432.0493 - val_loss: 2669002.4582\n",
      "Epoch 49/100\n",
      "2583/2583 [==============================] - 0s 152us/step - loss: 2312595.9326 - val_loss: 2669428.3065\n",
      "Epoch 50/100\n",
      "2583/2583 [==============================] - 0s 142us/step - loss: 2307245.6193 - val_loss: 2664333.2268\n",
      "Epoch 51/100\n",
      "2583/2583 [==============================] - 0s 122us/step - loss: 2302369.0549 - val_loss: 2664025.9450\n",
      "Epoch 52/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2296382.3690 - val_loss: 2663869.1378\n",
      "Epoch 53/100\n",
      "2583/2583 [==============================] - 0s 117us/step - loss: 2292067.7283 - val_loss: 2661352.6974\n",
      "Epoch 54/100\n",
      "2583/2583 [==============================] - 0s 117us/step - loss: 2288050.3615 - val_loss: 2660773.2717\n",
      "Epoch 55/100\n",
      "2583/2583 [==============================] - 0s 131us/step - loss: 2282482.8687 - val_loss: 2660689.1610\n",
      "Epoch 56/100\n",
      "2583/2583 [==============================] - 0s 119us/step - loss: 2277724.7563 - val_loss: 2660628.8622\n",
      "Epoch 57/100\n",
      "2583/2583 [==============================] - 0s 123us/step - loss: 2274814.0554 - val_loss: 2658080.9071\n",
      "Epoch 58/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2267498.9590 - val_loss: 2660801.9249\n",
      "Epoch 59/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2263979.7504 - val_loss: 2657970.5170\n",
      "Epoch 60/100\n",
      "2583/2583 [==============================] - 0s 117us/step - loss: 2259619.9022 - val_loss: 2658730.5789\n",
      "Epoch 61/100\n",
      "2583/2583 [==============================] - 0s 112us/step - loss: 2254075.7580 - val_loss: 2655786.8762\n",
      "Epoch 62/100\n",
      "2583/2583 [==============================] - 0s 115us/step - loss: 2249703.1307 - val_loss: 2655422.0550\n",
      "Epoch 63/100\n",
      "2583/2583 [==============================] - 0s 115us/step - loss: 2244821.1023 - val_loss: 2657616.1950\n",
      "Epoch 64/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2241670.8041 - val_loss: 2656381.4628\n",
      "Epoch 65/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2236653.9260 - val_loss: 2654193.7910\n",
      "Epoch 66/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 2232422.6027 - val_loss: 2654754.5224\n",
      "Epoch 67/100\n",
      "2583/2583 [==============================] - 0s 114us/step - loss: 2228456.7141 - val_loss: 2655267.0635\n",
      "Epoch 68/100\n",
      "2583/2583 [==============================] - 0s 126us/step - loss: 2222893.2818 - val_loss: 2656896.3901\n",
      "Epoch 69/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2219576.9197 - val_loss: 2658435.3050\n",
      "Epoch 70/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2216130.3757 - val_loss: 2655332.7686\n",
      "Epoch 71/100\n",
      "2583/2583 [==============================] - 0s 112us/step - loss: 2211402.3983 - val_loss: 2656693.8491\n",
      "Epoch 72/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2208440.3652 - val_loss: 2654890.8707\n",
      "Epoch 73/100\n",
      "2583/2583 [==============================] - 0s 112us/step - loss: 2202989.0610 - val_loss: 2655485.2740\n",
      "Epoch 74/100\n",
      "2583/2583 [==============================] - 0s 115us/step - loss: 2199706.6394 - val_loss: 2657415.3173\n",
      "Epoch 75/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2195255.1096 - val_loss: 2658759.3994\n",
      "1292/1292 [==============================] - 0s 43us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 239us/step - loss: 3566660.6930 - val_loss: 3802494.9040\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 131us/step - loss: 3391679.6637 - val_loss: 3436666.6022\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 126us/step - loss: 3026941.4322 - val_loss: 3102054.8615\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2827316.6707 - val_loss: 3011306.3034\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 117us/step - loss: 2787526.5627 - val_loss: 2982423.7872\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 115us/step - loss: 2754742.9365 - val_loss: 2953130.6703\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 115us/step - loss: 2720632.6314 - val_loss: 2922723.6424\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2685784.1342 - val_loss: 2892037.7717\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 120us/step - loss: 2651861.1507 - val_loss: 2866899.8421\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 112us/step - loss: 2621803.1340 - val_loss: 2840625.1223\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 120us/step - loss: 2593401.2528 - val_loss: 2819677.7067\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2569379.6490 - val_loss: 2802109.5720\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2547466.6140 - val_loss: 2785969.4946\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 122us/step - loss: 2528694.4105 - val_loss: 2772939.6865\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2512836.4434 - val_loss: 2762374.8080\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 108us/step - loss: 2499303.1357 - val_loss: 2753371.0410\n",
      "Epoch 17/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2487217.8424 - val_loss: 2747268.4613\n",
      "Epoch 18/100\n",
      "2583/2583 [==============================] - 0s 112us/step - loss: 2476287.0694 - val_loss: 2738746.1811\n",
      "Epoch 19/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2466546.8542 - val_loss: 2732887.5991\n",
      "Epoch 20/100\n",
      "2583/2583 [==============================] - 0s 117us/step - loss: 2457435.0134 - val_loss: 2728175.6633\n",
      "Epoch 21/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2449030.8281 - val_loss: 2722593.0240\n",
      "Epoch 22/100\n",
      "2583/2583 [==============================] - 0s 115us/step - loss: 2440337.9671 - val_loss: 2718063.6726\n",
      "Epoch 23/100\n",
      "2583/2583 [==============================] - 0s 115us/step - loss: 2431902.5763 - val_loss: 2713703.9598\n",
      "Epoch 24/100\n",
      "2583/2583 [==============================] - 0s 108us/step - loss: 2424307.1961 - val_loss: 2708896.4899\n",
      "Epoch 25/100\n",
      "2583/2583 [==============================] - 0s 119us/step - loss: 2417471.3490 - val_loss: 2705256.7407\n",
      "Epoch 26/100\n",
      "2583/2583 [==============================] - 0s 128us/step - loss: 2409579.0415 - val_loss: 2701206.8769\n",
      "Epoch 27/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2402343.3160 - val_loss: 2696472.2237\n",
      "Epoch 28/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2394852.1513 - val_loss: 2692999.9505\n",
      "Epoch 29/100\n",
      "2583/2583 [==============================] - 0s 133us/step - loss: 2389807.8505 - val_loss: 2690592.2121\n",
      "Epoch 30/100\n",
      "2583/2583 [==============================] - 0s 134us/step - loss: 2380899.7938 - val_loss: 2685899.0929\n",
      "Epoch 31/100\n",
      "2583/2583 [==============================] - 0s 143us/step - loss: 2374694.8277 - val_loss: 2683975.8127\n",
      "Epoch 32/100\n",
      "2583/2583 [==============================] - 0s 111us/step - loss: 2367234.4215 - val_loss: 2680270.6850\n",
      "Epoch 33/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2359946.6342 - val_loss: 2676897.9288\n",
      "Epoch 34/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2353539.1721 - val_loss: 2673450.2105\n",
      "Epoch 35/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2347565.7901 - val_loss: 2670547.8475\n",
      "Epoch 36/100\n",
      "2583/2583 [==============================] - 0s 123us/step - loss: 2341072.6161 - val_loss: 2669526.0031\n",
      "Epoch 37/100\n",
      "2583/2583 [==============================] - 0s 116us/step - loss: 2335013.3489 - val_loss: 2664902.8073\n",
      "Epoch 38/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2329009.4408 - val_loss: 2663107.0410\n",
      "Epoch 39/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2322476.2965 - val_loss: 2660988.2260\n",
      "Epoch 40/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2316497.8413 - val_loss: 2658500.5062\n",
      "Epoch 41/100\n",
      "2583/2583 [==============================] - 0s 117us/step - loss: 2308955.8845 - val_loss: 2656869.3336\n",
      "Epoch 42/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2304318.8655 - val_loss: 2654789.4543\n",
      "Epoch 43/100\n",
      "2583/2583 [==============================] - 0s 131us/step - loss: 2298070.4005 - val_loss: 2653398.7423\n",
      "Epoch 44/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2290977.4771 - val_loss: 2650583.1285\n",
      "Epoch 45/100\n",
      "2583/2583 [==============================] - 0s 111us/step - loss: 2285358.7938 - val_loss: 2649554.7190\n",
      "Epoch 46/100\n",
      "2583/2583 [==============================] - 0s 146us/step - loss: 2279448.8799 - val_loss: 2648205.8707\n",
      "Epoch 47/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2274021.7863 - val_loss: 2646568.4094\n",
      "Epoch 48/100\n",
      "2583/2583 [==============================] - 0s 125us/step - loss: 2268183.8378 - val_loss: 2647466.9404\n",
      "Epoch 49/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2262939.7106 - val_loss: 2643440.1393\n",
      "Epoch 50/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2256821.6815 - val_loss: 2642713.6958\n",
      "Epoch 51/100\n",
      "2583/2583 [==============================] - 0s 108us/step - loss: 2251756.2034 - val_loss: 2642112.1262\n",
      "Epoch 52/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2246213.1880 - val_loss: 2640685.8723\n",
      "Epoch 53/100\n",
      "2583/2583 [==============================] - 0s 108us/step - loss: 2240852.2328 - val_loss: 2639696.0279\n",
      "Epoch 54/100\n",
      "2583/2583 [==============================] - 0s 106us/step - loss: 2235434.4291 - val_loss: 2639270.1045\n",
      "Epoch 55/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2229703.7627 - val_loss: 2638386.0673\n",
      "Epoch 56/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2224791.7387 - val_loss: 2638832.3220\n",
      "Epoch 57/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2219945.1345 - val_loss: 2637549.0155\n",
      "Epoch 58/100\n",
      "2583/2583 [==============================] - 0s 125us/step - loss: 2214957.2570 - val_loss: 2637136.8955\n",
      "Epoch 59/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2210301.0164 - val_loss: 2636700.3785\n",
      "Epoch 60/100\n",
      "2583/2583 [==============================] - 0s 112us/step - loss: 2205281.1617 - val_loss: 2637655.7353\n",
      "Epoch 61/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2200531.5362 - val_loss: 2636034.3669\n",
      "Epoch 62/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2196894.8079 - val_loss: 2636834.5186\n",
      "Epoch 63/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2192159.4414 - val_loss: 2637759.5673\n",
      "Epoch 64/100\n",
      "2583/2583 [==============================] - 0s 114us/step - loss: 2187558.2005 - val_loss: 2636362.4536\n",
      "Epoch 65/100\n",
      "2583/2583 [==============================] - 0s 114us/step - loss: 2180329.3360 - val_loss: 2635817.9327\n",
      "Epoch 66/100\n",
      "2583/2583 [==============================] - 0s 125us/step - loss: 2178829.8223 - val_loss: 2635911.4241\n",
      "Epoch 67/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2172423.1341 - val_loss: 2635999.8824\n",
      "Epoch 68/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2168026.3603 - val_loss: 2636708.5650\n",
      "Epoch 69/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2164384.3745 - val_loss: 2636775.8607\n",
      "Epoch 70/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2159137.7477 - val_loss: 2637790.4551\n",
      "Epoch 71/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2156016.2974 - val_loss: 2637780.9381\n",
      "Epoch 72/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2150673.4609 - val_loss: 2637530.2268\n",
      "Epoch 73/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 2147169.8348 - val_loss: 2638033.7020\n",
      "Epoch 74/100\n",
      "2583/2583 [==============================] - 0s 107us/step - loss: 2143478.3853 - val_loss: 2638855.0697\n",
      "Epoch 75/100\n",
      "2583/2583 [==============================] - 0s 107us/step - loss: 2139474.0228 - val_loss: 2639009.1633\n",
      "1292/1292 [==============================] - 0s 38us/step\n",
      "Train on 2584 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2584/2584 [==============================] - 1s 252us/step - loss: 3544113.8166 - val_loss: 3644437.7245\n",
      "Epoch 2/100\n",
      "2584/2584 [==============================] - 0s 149us/step - loss: 3321740.5408 - val_loss: 3334571.4211\n",
      "Epoch 3/100\n",
      "2584/2584 [==============================] - 0s 116us/step - loss: 3052299.7009 - val_loss: 3091953.4102\n",
      "Epoch 4/100\n",
      "2584/2584 [==============================] - 0s 115us/step - loss: 2919368.3390 - val_loss: 3023464.3189\n",
      "Epoch 5/100\n",
      "2584/2584 [==============================] - 0s 112us/step - loss: 2883823.3019 - val_loss: 2997706.5085\n",
      "Epoch 6/100\n",
      "2584/2584 [==============================] - 0s 117us/step - loss: 2860482.3104 - val_loss: 2976161.9040\n",
      "Epoch 7/100\n",
      "2584/2584 [==============================] - 0s 113us/step - loss: 2834456.3661 - val_loss: 2952276.3104\n",
      "Epoch 8/100\n",
      "2584/2584 [==============================] - 0s 108us/step - loss: 2805475.7779 - val_loss: 2925453.1695\n",
      "Epoch 9/100\n",
      "2584/2584 [==============================] - 0s 111us/step - loss: 2775229.6370 - val_loss: 2898799.1192\n",
      "Epoch 10/100\n",
      "2584/2584 [==============================] - 0s 110us/step - loss: 2746255.4203 - val_loss: 2874191.5542\n",
      "Epoch 11/100\n",
      "2584/2584 [==============================] - 0s 108us/step - loss: 2717456.1142 - val_loss: 2850338.0820\n",
      "Epoch 12/100\n",
      "2584/2584 [==============================] - 0s 117us/step - loss: 2690879.6037 - val_loss: 2828449.7810\n",
      "Epoch 13/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2668865.4663 - val_loss: 2811452.0588\n",
      "Epoch 14/100\n",
      "2584/2584 [==============================] - 0s 109us/step - loss: 2647840.9783 - val_loss: 2795783.8522\n",
      "Epoch 15/100\n",
      "2584/2584 [==============================] - 0s 108us/step - loss: 2629158.5246 - val_loss: 2780441.8870\n",
      "Epoch 16/100\n",
      "2584/2584 [==============================] - 0s 110us/step - loss: 2613722.6683 - val_loss: 2768565.5054\n",
      "Epoch 17/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2599815.5660 - val_loss: 2758706.4226\n",
      "Epoch 18/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2587751.5178 - val_loss: 2750348.7361\n",
      "Epoch 19/100\n",
      "2584/2584 [==============================] - 0s 106us/step - loss: 2575974.2968 - val_loss: 2741748.0418\n",
      "Epoch 20/100\n",
      "2584/2584 [==============================] - 0s 104us/step - loss: 2565267.9042 - val_loss: 2735228.2399\n",
      "Epoch 21/100\n",
      "2584/2584 [==============================] - 0s 107us/step - loss: 2555838.7086 - val_loss: 2729525.5356\n",
      "Epoch 22/100\n",
      "2584/2584 [==============================] - 0s 109us/step - loss: 2547292.1399 - val_loss: 2723393.5108\n",
      "Epoch 23/100\n",
      "2584/2584 [==============================] - 0s 116us/step - loss: 2539426.3522 - val_loss: 2717397.5364\n",
      "Epoch 24/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2531161.3493 - val_loss: 2712931.4180\n",
      "Epoch 25/100\n",
      "2584/2584 [==============================] - 0s 107us/step - loss: 2524062.4505 - val_loss: 2709572.4381\n",
      "Epoch 26/100\n",
      "2584/2584 [==============================] - 0s 104us/step - loss: 2517066.4570 - val_loss: 2705002.1037\n",
      "Epoch 27/100\n",
      "2584/2584 [==============================] - 0s 106us/step - loss: 2510569.7804 - val_loss: 2699922.4598\n",
      "Epoch 28/100\n",
      "2584/2584 [==============================] - 0s 109us/step - loss: 2504127.6343 - val_loss: 2696994.1656\n",
      "Epoch 29/100\n",
      "2584/2584 [==============================] - 0s 110us/step - loss: 2497280.8711 - val_loss: 2693588.7670\n",
      "Epoch 30/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2491541.7136 - val_loss: 2690764.1711\n",
      "Epoch 31/100\n",
      "2584/2584 [==============================] - 0s 104us/step - loss: 2485689.9025 - val_loss: 2688187.9845\n",
      "Epoch 32/100\n",
      "2584/2584 [==============================] - 0s 112us/step - loss: 2479018.5224 - val_loss: 2684597.6556\n",
      "Epoch 33/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2472123.2796 - val_loss: 2681542.9450\n",
      "Epoch 34/100\n",
      "2584/2584 [==============================] - 0s 106us/step - loss: 2466347.0333 - val_loss: 2679862.0611\n",
      "Epoch 35/100\n",
      "2584/2584 [==============================] - 0s 102us/step - loss: 2460353.6339 - val_loss: 2676628.9706\n",
      "Epoch 36/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2454222.5104 - val_loss: 2674517.4551\n",
      "Epoch 37/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2448697.5528 - val_loss: 2670276.0015\n",
      "Epoch 38/100\n",
      "2584/2584 [==============================] - 0s 107us/step - loss: 2442365.7376 - val_loss: 2669398.8646\n",
      "Epoch 39/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2437258.4644 - val_loss: 2667816.7748\n",
      "Epoch 40/100\n",
      "2584/2584 [==============================] - 0s 113us/step - loss: 2430375.3731 - val_loss: 2664309.4690\n",
      "Epoch 41/100\n",
      "2584/2584 [==============================] - 0s 127us/step - loss: 2425123.9307 - val_loss: 2662066.8599\n",
      "Epoch 42/100\n",
      "2584/2584 [==============================] - 0s 140us/step - loss: 2420386.9752 - val_loss: 2660007.6200\n",
      "Epoch 43/100\n",
      "2584/2584 [==============================] - 0s 104us/step - loss: 2415456.7873 - val_loss: 2660163.6680\n",
      "Epoch 44/100\n",
      "2584/2584 [==============================] - 0s 116us/step - loss: 2408860.0420 - val_loss: 2658008.8096\n",
      "Epoch 45/100\n",
      "2584/2584 [==============================] - 0s 116us/step - loss: 2403562.4888 - val_loss: 2656432.8297\n",
      "Epoch 46/100\n",
      "2584/2584 [==============================] - 0s 108us/step - loss: 2399373.2837 - val_loss: 2655866.6896\n",
      "Epoch 47/100\n",
      "2584/2584 [==============================] - 0s 106us/step - loss: 2392609.9659 - val_loss: 2653034.1207\n",
      "Epoch 48/100\n",
      "2584/2584 [==============================] - 0s 134us/step - loss: 2388166.4570 - val_loss: 2654058.3700\n",
      "Epoch 49/100\n",
      "2584/2584 [==============================] - 0s 108us/step - loss: 2383366.3170 - val_loss: 2649948.4605\n",
      "Epoch 50/100\n",
      "2584/2584 [==============================] - 0s 146us/step - loss: 2379052.7639 - val_loss: 2648971.8978\n",
      "Epoch 51/100\n",
      "2584/2584 [==============================] - 0s 122us/step - loss: 2373035.5046 - val_loss: 2647925.9644\n",
      "Epoch 52/100\n",
      "2584/2584 [==============================] - 0s 109us/step - loss: 2368138.9063 - val_loss: 2646840.5588\n",
      "Epoch 53/100\n",
      "2584/2584 [==============================] - 0s 125us/step - loss: 2366104.0197 - val_loss: 2645388.1099\n",
      "Epoch 54/100\n",
      "2584/2584 [==============================] - 0s 143us/step - loss: 2359736.0716 - val_loss: 2644140.6525\n",
      "Epoch 55/100\n",
      "2584/2584 [==============================] - 0s 126us/step - loss: 2354265.8707 - val_loss: 2643374.3019\n",
      "Epoch 56/100\n",
      "2584/2584 [==============================] - 0s 112us/step - loss: 2350186.2500 - val_loss: 2645288.9528\n",
      "Epoch 57/100\n",
      "2584/2584 [==============================] - 0s 120us/step - loss: 2344770.4152 - val_loss: 2642555.0867\n",
      "Epoch 58/100\n",
      "2584/2584 [==============================] - 0s 141us/step - loss: 2340515.6958 - val_loss: 2642869.3243\n",
      "Epoch 59/100\n",
      "2584/2584 [==============================] - 0s 163us/step - loss: 2336424.1805 - val_loss: 2642044.8483\n",
      "Epoch 60/100\n",
      "2584/2584 [==============================] - 0s 125us/step - loss: 2331311.7632 - val_loss: 2641440.3483\n",
      "Epoch 61/100\n",
      "2584/2584 [==============================] - 0s 114us/step - loss: 2327936.6428 - val_loss: 2641761.1749\n",
      "Epoch 62/100\n",
      "2584/2584 [==============================] - 0s 110us/step - loss: 2322728.7848 - val_loss: 2640825.5635\n",
      "Epoch 63/100\n",
      "2584/2584 [==============================] - 0s 103us/step - loss: 2319203.2330 - val_loss: 2642377.7190\n",
      "Epoch 64/100\n",
      "2584/2584 [==============================] - 0s 103us/step - loss: 2314773.9205 - val_loss: 2640764.0201\n",
      "Epoch 65/100\n",
      "2584/2584 [==============================] - 0s 108us/step - loss: 2311551.3545 - val_loss: 2641528.5875\n",
      "Epoch 66/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2307637.9007 - val_loss: 2641560.9450\n",
      "Epoch 67/100\n",
      "2584/2584 [==============================] - 0s 103us/step - loss: 2301366.9005 - val_loss: 2644049.4690\n",
      "Epoch 68/100\n",
      "2584/2584 [==============================] - 0s 106us/step - loss: 2297843.9923 - val_loss: 2640887.0952\n",
      "Epoch 69/100\n",
      "2584/2584 [==============================] - 0s 106us/step - loss: 2294476.7156 - val_loss: 2642715.4992\n",
      "Epoch 70/100\n",
      "2584/2584 [==============================] - 0s 102us/step - loss: 2289031.2515 - val_loss: 2641582.2709\n",
      "Epoch 71/100\n",
      "2584/2584 [==============================] - 0s 118us/step - loss: 2286588.9830 - val_loss: 2644700.7995\n",
      "Epoch 72/100\n",
      "2584/2584 [==============================] - 0s 104us/step - loss: 2281013.6271 - val_loss: 2642817.0658\n",
      "Epoch 73/100\n",
      "2584/2584 [==============================] - 0s 103us/step - loss: 2277943.1169 - val_loss: 2643383.6393\n",
      "Epoch 74/100\n",
      "2584/2584 [==============================] - 0s 103us/step - loss: 2274290.7968 - val_loss: 2645754.6246\n",
      "1291/1291 [==============================] - 0s 37us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 219us/step - loss: 3551322.6009 - val_loss: 3817602.5139\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 3486460.6012 - val_loss: 3692437.7647\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 90us/step - loss: 3372281.2097 - val_loss: 3565045.2972\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 86us/step - loss: 3249701.1570 - val_loss: 3433174.4938\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 86us/step - loss: 3123481.9484 - val_loss: 3298535.6672\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 88us/step - loss: 3011307.6086 - val_loss: 3183920.5232\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2920654.0812 - val_loss: 3104886.1200\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2867428.7279 - val_loss: 3061975.2709\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 98us/step - loss: 2840204.3700 - val_loss: 3039440.6122\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 81us/step - loss: 2826289.0376 - val_loss: 3025370.3491\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2817624.5640 - val_loss: 3017628.4226\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2811001.5587 - val_loss: 3012079.6378\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 83us/step - loss: 2804084.0817 - val_loss: 3006063.2709\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2797412.3243 - val_loss: 3000791.5310\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 81us/step - loss: 2790782.5919 - val_loss: 2995255.3568\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 78us/step - loss: 2784362.7107 - val_loss: 2989535.1958\n",
      "Epoch 17/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2777414.1708 - val_loss: 2984384.5224\n",
      "Epoch 18/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2770805.7990 - val_loss: 2978431.9969\n",
      "Epoch 19/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2763491.6175 - val_loss: 2972733.7098\n",
      "Epoch 20/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2755362.6937 - val_loss: 2966179.3576\n",
      "Epoch 21/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2746648.4469 - val_loss: 2958983.9280\n",
      "Epoch 22/100\n",
      "2583/2583 [==============================] - 0s 82us/step - loss: 2737957.3559 - val_loss: 2951560.1231\n",
      "Epoch 23/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2729120.0321 - val_loss: 2943915.3847\n",
      "Epoch 24/100\n",
      "2583/2583 [==============================] - 0s 92us/step - loss: 2719859.0227 - val_loss: 2936811.7469\n",
      "Epoch 25/100\n",
      "2583/2583 [==============================] - 0s 81us/step - loss: 2710905.2402 - val_loss: 2929033.9489\n",
      "Epoch 26/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2701372.7568 - val_loss: 2921713.1014\n",
      "Epoch 27/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2692202.7901 - val_loss: 2914575.2121\n",
      "Epoch 28/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2683017.7356 - val_loss: 2907631.9118\n",
      "Epoch 29/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2674011.2666 - val_loss: 2900397.8545\n",
      "Epoch 30/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2665113.5879 - val_loss: 2893461.1060\n",
      "Epoch 31/100\n",
      "2583/2583 [==============================] - 0s 81us/step - loss: 2656441.2184 - val_loss: 2886870.3351\n",
      "Epoch 32/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2647723.0230 - val_loss: 2880663.0642\n",
      "Epoch 33/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2639598.7471 - val_loss: 2874080.3305\n",
      "Epoch 34/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2631134.8650 - val_loss: 2868521.0325\n",
      "Epoch 35/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2623476.6894 - val_loss: 2861618.3560\n",
      "Epoch 36/100\n",
      "2583/2583 [==============================] - 0s 91us/step - loss: 2615228.4322 - val_loss: 2856223.8026\n",
      "Epoch 37/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2607563.4126 - val_loss: 2850605.9489\n",
      "Epoch 38/100\n",
      "2583/2583 [==============================] - 0s 91us/step - loss: 2600113.2077 - val_loss: 2845127.0132\n",
      "Epoch 39/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2592820.8798 - val_loss: 2840696.5766\n",
      "Epoch 40/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2586118.5144 - val_loss: 2835000.0488\n",
      "Epoch 41/100\n",
      "2583/2583 [==============================] - 0s 81us/step - loss: 2578929.4838 - val_loss: 2830884.2995\n",
      "Epoch 42/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2572207.4609 - val_loss: 2825992.5766\n",
      "Epoch 43/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2566297.9034 - val_loss: 2821055.1687\n",
      "Epoch 44/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2559162.6177 - val_loss: 2816961.0844\n",
      "Epoch 45/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2552614.2257 - val_loss: 2812953.4172\n",
      "Epoch 46/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2546372.8007 - val_loss: 2808911.0457\n",
      "Epoch 47/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2540515.8287 - val_loss: 2804362.4652\n",
      "Epoch 48/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2534457.8433 - val_loss: 2800180.1269\n",
      "Epoch 49/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2528747.6280 - val_loss: 2797656.7988\n",
      "Epoch 50/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2523186.5473 - val_loss: 2793563.6664\n",
      "Epoch 51/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2517828.4016 - val_loss: 2790407.6486\n",
      "Epoch 52/100\n",
      "2583/2583 [==============================] - 0s 88us/step - loss: 2512782.6507 - val_loss: 2787050.2895\n",
      "Epoch 53/100\n",
      "2583/2583 [==============================] - 0s 81us/step - loss: 2507902.5114 - val_loss: 2784272.0573\n",
      "Epoch 54/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2502796.8853 - val_loss: 2781311.9752\n",
      "Epoch 55/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2498615.0034 - val_loss: 2778625.1533\n",
      "Epoch 56/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2494018.9309 - val_loss: 2775782.7887\n",
      "Epoch 57/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2488895.2250 - val_loss: 2773175.2624\n",
      "Epoch 58/100\n",
      "2583/2583 [==============================] - 0s 82us/step - loss: 2484802.4357 - val_loss: 2770673.3367\n",
      "Epoch 59/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2480944.8147 - val_loss: 2767722.9125\n",
      "Epoch 60/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2476321.8829 - val_loss: 2766125.5093\n",
      "Epoch 61/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2472512.8466 - val_loss: 2763803.2012\n",
      "Epoch 62/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2468616.1434 - val_loss: 2762240.8003\n",
      "Epoch 63/100\n",
      "2583/2583 [==============================] - 0s 81us/step - loss: 2464958.0745 - val_loss: 2759948.4807\n",
      "Epoch 64/100\n",
      "2583/2583 [==============================] - 0s 89us/step - loss: 2461384.7558 - val_loss: 2757594.9652\n",
      "Epoch 65/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2457920.2877 - val_loss: 2756652.3824\n",
      "Epoch 66/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2454177.5093 - val_loss: 2754473.2810\n",
      "Epoch 67/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2450868.4219 - val_loss: 2752132.7028\n",
      "Epoch 68/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2447633.9175 - val_loss: 2750613.3413\n",
      "Epoch 69/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2444917.0078 - val_loss: 2748698.2848\n",
      "Epoch 70/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2440829.4808 - val_loss: 2747972.5186\n",
      "Epoch 71/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2437492.5643 - val_loss: 2745836.7546\n",
      "Epoch 72/100\n",
      "2583/2583 [==============================] - 0s 78us/step - loss: 2434227.3916 - val_loss: 2744343.8584\n",
      "Epoch 73/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2431304.7602 - val_loss: 2742976.5650\n",
      "Epoch 74/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2427934.6043 - val_loss: 2742006.6974\n",
      "Epoch 75/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2425069.9758 - val_loss: 2740009.4474\n",
      "Epoch 76/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2421992.9669 - val_loss: 2738616.9102\n",
      "Epoch 77/100\n",
      "2583/2583 [==============================] - 0s 91us/step - loss: 2418987.4601 - val_loss: 2737293.7059\n",
      "Epoch 78/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2416101.1647 - val_loss: 2736498.3831\n",
      "Epoch 79/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2412613.1969 - val_loss: 2734264.1486\n",
      "Epoch 80/100\n",
      "2583/2583 [==============================] - 0s 81us/step - loss: 2409968.7643 - val_loss: 2732949.4961\n",
      "Epoch 81/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2407194.4287 - val_loss: 2732203.7283\n",
      "Epoch 82/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2404900.1260 - val_loss: 2731409.0077\n",
      "Epoch 83/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2401316.1508 - val_loss: 2730057.2748\n",
      "Epoch 84/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2399282.0192 - val_loss: 2728717.1486\n",
      "Epoch 85/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2395447.5374 - val_loss: 2727066.8731\n",
      "Epoch 86/100\n",
      "2583/2583 [==============================] - 0s 78us/step - loss: 2392602.0545 - val_loss: 2725695.8800\n",
      "Epoch 87/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2391032.9781 - val_loss: 2725465.8266\n",
      "Epoch 88/100\n",
      "2583/2583 [==============================] - 0s 81us/step - loss: 2387491.5778 - val_loss: 2724273.0828\n",
      "Epoch 89/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2384586.0382 - val_loss: 2723752.3653\n",
      "Epoch 90/100\n",
      "2583/2583 [==============================] - 0s 90us/step - loss: 2381877.0876 - val_loss: 2721397.8460\n",
      "Epoch 91/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2378886.1986 - val_loss: 2721071.7012\n",
      "Epoch 92/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2376132.3485 - val_loss: 2719150.4830\n",
      "Epoch 93/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2374999.2200 - val_loss: 2718902.0000\n",
      "Epoch 94/100\n",
      "2583/2583 [==============================] - 0s 78us/step - loss: 2370968.6016 - val_loss: 2716879.4807\n",
      "Epoch 95/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2368176.4193 - val_loss: 2716036.3560\n",
      "Epoch 96/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2365591.7879 - val_loss: 2715226.3769\n",
      "Epoch 97/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2362614.9959 - val_loss: 2714512.7144\n",
      "Epoch 98/100\n",
      "2583/2583 [==============================] - 0s 81us/step - loss: 2359807.7780 - val_loss: 2713400.1656\n",
      "Epoch 99/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2357192.3721 - val_loss: 2712586.7562\n",
      "Epoch 100/100\n",
      "2583/2583 [==============================] - 0s 79us/step - loss: 2355400.0082 - val_loss: 2712675.8119\n",
      "1292/1292 [==============================] - 0s 29us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 217us/step - loss: 3529203.0976 - val_loss: 3751658.2152\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 109us/step - loss: 3470487.4563 - val_loss: 3679431.9303\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 3380675.2462 - val_loss: 3567294.5124\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 87us/step - loss: 3248599.9918 - val_loss: 3404545.2152\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 3094938.7414 - val_loss: 3247533.8158\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2956840.9049 - val_loss: 3118204.9226\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 88us/step - loss: 2864795.1772 - val_loss: 3052013.1602\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2825995.2865 - val_loss: 3028201.8475\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 89us/step - loss: 2813573.3925 - val_loss: 3019566.8638\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 86us/step - loss: 2805774.3963 - val_loss: 3012257.0279\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2798571.6153 - val_loss: 3006431.0836\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2791713.6719 - val_loss: 3000987.2825\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2784536.0591 - val_loss: 2995179.9528\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2777606.1810 - val_loss: 2989618.8282\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2770405.2208 - val_loss: 2983387.7802\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2764326.2511 - val_loss: 2977859.4698\n",
      "Epoch 17/100\n",
      "2583/2583 [==============================] - 0s 87us/step - loss: 2756219.5573 - val_loss: 2972957.6045\n",
      "Epoch 18/100\n",
      "2583/2583 [==============================] - 0s 89us/step - loss: 2749031.7788 - val_loss: 2966679.7515\n",
      "Epoch 19/100\n",
      "2583/2583 [==============================] - 0s 86us/step - loss: 2741737.1192 - val_loss: 2961138.4667\n",
      "Epoch 20/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2734811.9563 - val_loss: 2955560.8947\n",
      "Epoch 21/100\n",
      "2583/2583 [==============================] - 0s 90us/step - loss: 2727618.3066 - val_loss: 2949789.4497\n",
      "Epoch 22/100\n",
      "2583/2583 [==============================] - 0s 86us/step - loss: 2719938.2205 - val_loss: 2944092.7299\n",
      "Epoch 23/100\n",
      "2583/2583 [==============================] - 0s 86us/step - loss: 2713502.3032 - val_loss: 2939149.4915\n",
      "Epoch 24/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2705967.6907 - val_loss: 2933340.6432\n",
      "Epoch 25/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2698883.3448 - val_loss: 2928041.2748\n",
      "Epoch 26/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2691695.4937 - val_loss: 2922771.0952\n",
      "Epoch 27/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2685090.9588 - val_loss: 2917326.2175\n",
      "Epoch 28/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2678071.3243 - val_loss: 2911850.6184\n",
      "Epoch 29/100\n",
      "2583/2583 [==============================] - 0s 87us/step - loss: 2671167.7888 - val_loss: 2906799.5952\n",
      "Epoch 30/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2664228.2203 - val_loss: 2902067.7663\n",
      "Epoch 31/100\n",
      "2583/2583 [==============================] - 0s 87us/step - loss: 2658229.5925 - val_loss: 2897735.1293\n",
      "Epoch 32/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2650559.7832 - val_loss: 2892346.1711\n",
      "Epoch 33/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2644259.8582 - val_loss: 2887368.6045\n",
      "Epoch 34/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2637999.9115 - val_loss: 2882762.0983\n",
      "Epoch 35/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2630849.6134 - val_loss: 2878064.8019\n",
      "Epoch 36/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2624778.2167 - val_loss: 2873673.6680\n",
      "Epoch 37/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2618138.1121 - val_loss: 2868810.2841\n",
      "Epoch 38/100\n",
      "2583/2583 [==============================] - 0s 83us/step - loss: 2611968.0221 - val_loss: 2864488.8282\n",
      "Epoch 39/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2605767.7467 - val_loss: 2859756.7601\n",
      "Epoch 40/100\n",
      "2583/2583 [==============================] - 0s 90us/step - loss: 2599100.9011 - val_loss: 2855325.2918\n",
      "Epoch 41/100\n",
      "2583/2583 [==============================] - 0s 87us/step - loss: 2592864.1884 - val_loss: 2850966.4853\n",
      "Epoch 42/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2586971.4475 - val_loss: 2846744.2268\n",
      "Epoch 43/100\n",
      "2583/2583 [==============================] - 0s 87us/step - loss: 2581101.4661 - val_loss: 2842634.3947\n",
      "Epoch 44/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2575135.3519 - val_loss: 2838869.1385\n",
      "Epoch 45/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2569527.3635 - val_loss: 2834692.6819\n",
      "Epoch 46/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2563963.0094 - val_loss: 2831432.0410\n",
      "Epoch 47/100\n",
      "2583/2583 [==============================] - 0s 120us/step - loss: 2558033.1871 - val_loss: 2827456.5766\n",
      "Epoch 48/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2552977.7180 - val_loss: 2824187.1834\n",
      "Epoch 49/100\n",
      "2583/2583 [==============================] - 0s 87us/step - loss: 2547375.8340 - val_loss: 2820302.0085\n",
      "Epoch 50/100\n",
      "2583/2583 [==============================] - 0s 87us/step - loss: 2542343.2252 - val_loss: 2816772.3893\n",
      "Epoch 51/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2537155.8126 - val_loss: 2813530.3305\n",
      "Epoch 52/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2532436.4877 - val_loss: 2810287.4226\n",
      "Epoch 53/100\n",
      "2583/2583 [==============================] - 0s 90us/step - loss: 2527464.0397 - val_loss: 2807084.5410\n",
      "Epoch 54/100\n",
      "2583/2583 [==============================] - 0s 89us/step - loss: 2521893.5437 - val_loss: 2803370.9133\n",
      "Epoch 55/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2516599.7891 - val_loss: 2800188.3940\n",
      "Epoch 56/100\n",
      "2583/2583 [==============================] - 0s 88us/step - loss: 2512162.3899 - val_loss: 2796747.2624\n",
      "Epoch 57/100\n",
      "2583/2583 [==============================] - 0s 84us/step - loss: 2507003.6990 - val_loss: 2794332.1981\n",
      "Epoch 58/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2501869.3885 - val_loss: 2791799.9094\n",
      "Epoch 59/100\n",
      "2583/2583 [==============================] - 0s 85us/step - loss: 2497184.2488 - val_loss: 2788336.3986\n",
      "Epoch 60/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2492847.5271 - val_loss: 2785486.0844\n",
      "Epoch 61/100\n",
      "2583/2583 [==============================] - 0s 80us/step - loss: 2487615.3589 - val_loss: 2782876.5511\n",
      "Epoch 62/100\n",
      "2583/2583 [==============================] - 0s 89us/step - loss: 2483439.0379 - val_loss: 2779921.6672\n",
      "Epoch 63/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2479013.0286 - val_loss: 2777606.1045\n",
      "Epoch 64/100\n",
      "2583/2583 [==============================] - 0s 126us/step - loss: 2474923.5238 - val_loss: 2775138.9543\n",
      "Epoch 65/100\n",
      "2583/2583 [==============================] - 0s 100us/step - loss: 2470445.9925 - val_loss: 2773231.2670\n",
      "Epoch 66/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2466169.8019 - val_loss: 2770676.8096\n",
      "Epoch 67/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2462432.9341 - val_loss: 2767958.5983\n",
      "Epoch 68/100\n",
      "2583/2583 [==============================] - 0s 120us/step - loss: 2458404.5543 - val_loss: 2765544.0689\n",
      "Epoch 69/100\n",
      "2583/2583 [==============================] - 0s 108us/step - loss: 2454119.3775 - val_loss: 2763779.3220\n",
      "Epoch 70/100\n",
      "2583/2583 [==============================] - 0s 120us/step - loss: 2450564.6909 - val_loss: 2762694.4528\n",
      "Epoch 71/100\n",
      "2583/2583 [==============================] - 0s 130us/step - loss: 2446076.8321 - val_loss: 2759775.9791\n",
      "Epoch 72/100\n",
      "2583/2583 [==============================] - 0s 97us/step - loss: 2441967.6119 - val_loss: 2757434.8529\n",
      "Epoch 73/100\n",
      "2583/2583 [==============================] - 0s 145us/step - loss: 2438282.8172 - val_loss: 2755805.0441\n",
      "Epoch 74/100\n",
      "2583/2583 [==============================] - 0s 103us/step - loss: 2434595.7487 - val_loss: 2753452.3483\n",
      "Epoch 75/100\n",
      "2583/2583 [==============================] - 0s 110us/step - loss: 2431014.2843 - val_loss: 2751679.3421\n",
      "Epoch 76/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2427226.2384 - val_loss: 2750199.4536\n",
      "Epoch 77/100\n",
      "2583/2583 [==============================] - 0s 159us/step - loss: 2424118.0255 - val_loss: 2747669.0441\n",
      "Epoch 78/100\n",
      "2583/2583 [==============================] - 0s 91us/step - loss: 2420697.3400 - val_loss: 2745900.1308\n",
      "Epoch 79/100\n",
      "2583/2583 [==============================] - 0s 91us/step - loss: 2416901.1323 - val_loss: 2744161.4450\n",
      "Epoch 80/100\n",
      "2583/2583 [==============================] - 0s 93us/step - loss: 2413028.7323 - val_loss: 2742810.1997\n",
      "Epoch 81/100\n",
      "2583/2583 [==============================] - 0s 103us/step - loss: 2409545.5350 - val_loss: 2741182.2392\n",
      "Epoch 82/100\n",
      "2583/2583 [==============================] - 0s 103us/step - loss: 2406108.3116 - val_loss: 2739699.3769\n",
      "Epoch 83/100\n",
      "2583/2583 [==============================] - 0s 113us/step - loss: 2402475.1098 - val_loss: 2737803.9443\n",
      "Epoch 84/100\n",
      "2583/2583 [==============================] - 0s 99us/step - loss: 2399970.8600 - val_loss: 2736069.0132\n",
      "Epoch 85/100\n",
      "2583/2583 [==============================] - 0s 94us/step - loss: 2395224.3397 - val_loss: 2734353.6277\n",
      "Epoch 86/100\n",
      "2583/2583 [==============================] - 0s 90us/step - loss: 2392572.9156 - val_loss: 2733068.0302\n",
      "Epoch 87/100\n",
      "2583/2583 [==============================] - 0s 87us/step - loss: 2388848.3524 - val_loss: 2732224.8042\n",
      "Epoch 88/100\n",
      "2583/2583 [==============================] - 0s 88us/step - loss: 2385831.9553 - val_loss: 2730366.2616\n",
      "Epoch 89/100\n",
      "2583/2583 [==============================] - 0s 98us/step - loss: 2383060.0127 - val_loss: 2728661.9149\n",
      "Epoch 90/100\n",
      "2583/2583 [==============================] - 0s 102us/step - loss: 2379269.6350 - val_loss: 2727778.1401\n",
      "Epoch 91/100\n",
      "2583/2583 [==============================] - 0s 131us/step - loss: 2376859.7083 - val_loss: 2725692.0402\n",
      "Epoch 92/100\n",
      "2583/2583 [==============================] - 0s 118us/step - loss: 2372877.3464 - val_loss: 2724415.7392\n",
      "Epoch 93/100\n",
      "2583/2583 [==============================] - 0s 107us/step - loss: 2370283.7587 - val_loss: 2722681.7887\n",
      "Epoch 94/100\n",
      "2583/2583 [==============================] - 0s 94us/step - loss: 2366320.0156 - val_loss: 2722175.7539\n",
      "Epoch 95/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2363807.5254 - val_loss: 2720751.6447\n",
      "Epoch 96/100\n",
      "2583/2583 [==============================] - 0s 96us/step - loss: 2360556.2964 - val_loss: 2719377.7136\n",
      "Epoch 97/100\n",
      "2583/2583 [==============================] - 0s 94us/step - loss: 2356934.3401 - val_loss: 2718670.4822\n",
      "Epoch 98/100\n",
      "2583/2583 [==============================] - 0s 94us/step - loss: 2354017.3406 - val_loss: 2717024.8707\n",
      "Epoch 99/100\n",
      "2583/2583 [==============================] - 0s 93us/step - loss: 2351059.4614 - val_loss: 2715474.5588\n",
      "Epoch 100/100\n",
      "2583/2583 [==============================] - 0s 91us/step - loss: 2348279.1549 - val_loss: 2716074.6161\n",
      "1292/1292 [==============================] - 0s 26us/step\n",
      "Train on 2584 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2584/2584 [==============================] - 1s 244us/step - loss: 3617541.2082 - val_loss: 3789305.9474\n",
      "Epoch 2/100\n",
      "2584/2584 [==============================] - 0s 130us/step - loss: 3536776.0933 - val_loss: 3674304.1625\n",
      "Epoch 3/100\n",
      "2584/2584 [==============================] - 0s 96us/step - loss: 3422295.1478 - val_loss: 3529049.8111\n",
      "Epoch 4/100\n",
      "2584/2584 [==============================] - 0s 103us/step - loss: 3274667.7252 - val_loss: 3370077.1146\n",
      "Epoch 5/100\n",
      "2584/2584 [==============================] - 0s 118us/step - loss: 3137152.8562 - val_loss: 3213745.9474\n",
      "Epoch 6/100\n",
      "2584/2584 [==============================] - 0s 107us/step - loss: 3010729.0081 - val_loss: 3109745.7779\n",
      "Epoch 7/100\n",
      "2584/2584 [==============================] - 0s 85us/step - loss: 2943788.6242 - val_loss: 3058550.5875\n",
      "Epoch 8/100\n",
      "2584/2584 [==============================] - 0s 85us/step - loss: 2913854.7361 - val_loss: 3034034.8305\n",
      "Epoch 9/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2899675.5197 - val_loss: 3021614.7825\n",
      "Epoch 10/100\n",
      "2584/2584 [==============================] - 0s 84us/step - loss: 2890694.9249 - val_loss: 3013685.3243\n",
      "Epoch 11/100\n",
      "2584/2584 [==============================] - 0s 86us/step - loss: 2882750.9652 - val_loss: 3006822.9420\n",
      "Epoch 12/100\n",
      "2584/2584 [==============================] - 0s 84us/step - loss: 2875127.0561 - val_loss: 2999918.4373\n",
      "Epoch 13/100\n",
      "2584/2584 [==============================] - 0s 84us/step - loss: 2867489.0253 - val_loss: 2991908.5240\n",
      "Epoch 14/100\n",
      "2584/2584 [==============================] - 0s 85us/step - loss: 2858685.3352 - val_loss: 2984468.7995\n",
      "Epoch 15/100\n",
      "2584/2584 [==============================] - 0s 102us/step - loss: 2849777.4416 - val_loss: 2977247.6711\n",
      "Epoch 16/100\n",
      "2584/2584 [==============================] - 0s 93us/step - loss: 2841316.0580 - val_loss: 2970008.6153\n",
      "Epoch 17/100\n",
      "2584/2584 [==============================] - 0s 104us/step - loss: 2832305.7078 - val_loss: 2961976.3313\n",
      "Epoch 18/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2823573.4025 - val_loss: 2954562.3344\n",
      "Epoch 19/100\n",
      "2584/2584 [==============================] - 0s 86us/step - loss: 2814577.1842 - val_loss: 2946974.6927\n",
      "Epoch 20/100\n",
      "2584/2584 [==============================] - 0s 100us/step - loss: 2805777.1432 - val_loss: 2939926.4195\n",
      "Epoch 21/100\n",
      "2584/2584 [==============================] - 0s 166us/step - loss: 2797103.8769 - val_loss: 2931758.2608\n",
      "Epoch 22/100\n",
      "2584/2584 [==============================] - 1s 219us/step - loss: 2788331.5960 - val_loss: 2924628.5139\n",
      "Epoch 23/100\n",
      "2584/2584 [==============================] - 1s 202us/step - loss: 2779719.4365 - val_loss: 2917365.3367\n",
      "Epoch 24/100\n",
      "2584/2584 [==============================] - 0s 158us/step - loss: 2771002.4679 - val_loss: 2910502.6014\n",
      "Epoch 25/100\n",
      "2584/2584 [==============================] - 0s 168us/step - loss: 2763088.9118 - val_loss: 2903122.5433\n",
      "Epoch 26/100\n",
      "2584/2584 [==============================] - 1s 202us/step - loss: 2754232.3963 - val_loss: 2896359.9574\n",
      "Epoch 27/100\n",
      "2584/2584 [==============================] - 0s 164us/step - loss: 2745987.3541 - val_loss: 2889622.8429\n",
      "Epoch 28/100\n",
      "2584/2584 [==============================] - 0s 122us/step - loss: 2738292.9321 - val_loss: 2882350.3831\n",
      "Epoch 29/100\n",
      "2584/2584 [==============================] - 0s 92us/step - loss: 2730170.7389 - val_loss: 2876823.1881\n",
      "Epoch 30/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2722340.5410 - val_loss: 2869901.9776\n",
      "Epoch 31/100\n",
      "2584/2584 [==============================] - 0s 98us/step - loss: 2715129.8260 - val_loss: 2864512.3653\n",
      "Epoch 32/100\n",
      "2584/2584 [==============================] - 0s 104us/step - loss: 2707586.5132 - val_loss: 2857572.4605\n",
      "Epoch 33/100\n",
      "2584/2584 [==============================] - 0s 95us/step - loss: 2700375.5344 - val_loss: 2852135.3359\n",
      "Epoch 34/100\n",
      "2584/2584 [==============================] - 0s 93us/step - loss: 2693280.7372 - val_loss: 2846566.2856\n",
      "Epoch 35/100\n",
      "2584/2584 [==============================] - 0s 91us/step - loss: 2686388.8239 - val_loss: 2841158.5348\n",
      "Epoch 36/100\n",
      "2584/2584 [==============================] - 0s 92us/step - loss: 2679941.7508 - val_loss: 2835512.1215\n",
      "Epoch 37/100\n",
      "2584/2584 [==============================] - 0s 94us/step - loss: 2673160.4081 - val_loss: 2830248.8878\n",
      "Epoch 38/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2666747.8090 - val_loss: 2825531.8251\n",
      "Epoch 39/100\n",
      "2584/2584 [==============================] - 0s 100us/step - loss: 2660470.4470 - val_loss: 2820570.5859\n",
      "Epoch 40/100\n",
      "2584/2584 [==============================] - 0s 95us/step - loss: 2654834.7543 - val_loss: 2815714.9636\n",
      "Epoch 41/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2649409.8123 - val_loss: 2811478.3344\n",
      "Epoch 42/100\n",
      "2584/2584 [==============================] - 0s 82us/step - loss: 2642949.5205 - val_loss: 2807157.6161\n",
      "Epoch 43/100\n",
      "2584/2584 [==============================] - 0s 97us/step - loss: 2637586.6842 - val_loss: 2802841.9466\n",
      "Epoch 44/100\n",
      "2584/2584 [==============================] - 0s 88us/step - loss: 2632528.1707 - val_loss: 2798151.3769\n",
      "Epoch 45/100\n",
      "2584/2584 [==============================] - 0s 108us/step - loss: 2627177.2527 - val_loss: 2794550.7322\n",
      "Epoch 46/100\n",
      "2584/2584 [==============================] - 0s 94us/step - loss: 2621948.6188 - val_loss: 2790960.5039\n",
      "Epoch 47/100\n",
      "2584/2584 [==============================] - 0s 87us/step - loss: 2617030.8011 - val_loss: 2787133.4404\n",
      "Epoch 48/100\n",
      "2584/2584 [==============================] - 0s 148us/step - loss: 2612719.4814 - val_loss: 2784129.6695\n",
      "Epoch 49/100\n",
      "2584/2584 [==============================] - 0s 138us/step - loss: 2607458.2291 - val_loss: 2780121.2368\n",
      "Epoch 50/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2603332.0741 - val_loss: 2776926.9567\n",
      "Epoch 51/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2599778.1196 - val_loss: 2774220.3336\n",
      "Epoch 52/100\n",
      "2584/2584 [==============================] - 0s 105us/step - loss: 2594738.9803 - val_loss: 2770885.9334\n",
      "Epoch 53/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2590826.8108 - val_loss: 2767267.7663\n",
      "Epoch 54/100\n",
      "2584/2584 [==============================] - 0s 112us/step - loss: 2586244.5397 - val_loss: 2764526.4884\n",
      "Epoch 55/100\n",
      "2584/2584 [==============================] - 0s 150us/step - loss: 2582646.0395 - val_loss: 2762844.5836\n",
      "Epoch 56/100\n",
      "2584/2584 [==============================] - 0s 124us/step - loss: 2578752.4102 - val_loss: 2759626.9907\n",
      "Epoch 57/100\n",
      "2584/2584 [==============================] - 0s 114us/step - loss: 2574484.5453 - val_loss: 2756480.5317\n",
      "Epoch 58/100\n",
      "2584/2584 [==============================] - 0s 157us/step - loss: 2571010.4563 - val_loss: 2754052.9977\n",
      "Epoch 59/100\n",
      "2584/2584 [==============================] - 0s 102us/step - loss: 2567065.8746 - val_loss: 2751248.3437\n",
      "Epoch 60/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2563255.7399 - val_loss: 2749895.3584\n",
      "Epoch 61/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2560501.3324 - val_loss: 2746939.9621\n",
      "Epoch 62/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2556259.4296 - val_loss: 2744637.1068\n",
      "Epoch 63/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2552869.4257 - val_loss: 2742325.9961\n",
      "Epoch 64/100\n",
      "2584/2584 [==============================] - 0s 152us/step - loss: 2549329.8169 - val_loss: 2740032.8460\n",
      "Epoch 65/100\n",
      "2584/2584 [==============================] - 0s 150us/step - loss: 2545925.4957 - val_loss: 2738113.7647\n",
      "Epoch 66/100\n",
      "2584/2584 [==============================] - 0s 148us/step - loss: 2542916.3322 - val_loss: 2735852.9745\n",
      "Epoch 67/100\n",
      "2584/2584 [==============================] - 0s 101us/step - loss: 2539510.9849 - val_loss: 2733917.0402\n",
      "Epoch 68/100\n",
      "2584/2584 [==============================] - 0s 104us/step - loss: 2536020.8375 - val_loss: 2731632.1680\n",
      "Epoch 69/100\n",
      "2584/2584 [==============================] - 0s 123us/step - loss: 2533013.7697 - val_loss: 2730962.2585\n",
      "Epoch 70/100\n",
      "2584/2584 [==============================] - 0s 148us/step - loss: 2530304.5654 - val_loss: 2727626.6494\n",
      "Epoch 71/100\n",
      "2584/2584 [==============================] - 0s 150us/step - loss: 2526510.4632 - val_loss: 2726262.7686\n",
      "Epoch 72/100\n",
      "2584/2584 [==============================] - 0s 157us/step - loss: 2523771.6974 - val_loss: 2724575.8584\n",
      "Epoch 73/100\n",
      "2584/2584 [==============================] - 0s 88us/step - loss: 2520516.9370 - val_loss: 2722206.5101\n",
      "Epoch 74/100\n",
      "2584/2584 [==============================] - 0s 82us/step - loss: 2516803.5751 - val_loss: 2721096.0588\n",
      "Epoch 75/100\n",
      "2584/2584 [==============================] - 0s 97us/step - loss: 2514602.7353 - val_loss: 2718605.4133\n",
      "Epoch 76/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2510871.9528 - val_loss: 2716962.0132\n",
      "Epoch 77/100\n",
      "2584/2584 [==============================] - 0s 94us/step - loss: 2508452.7450 - val_loss: 2714909.9776\n",
      "Epoch 78/100\n",
      "2584/2584 [==============================] - 0s 89us/step - loss: 2505127.4849 - val_loss: 2713917.1819\n",
      "Epoch 79/100\n",
      "2584/2584 [==============================] - 0s 111us/step - loss: 2501843.6759 - val_loss: 2712686.3607\n",
      "Epoch 80/100\n",
      "2584/2584 [==============================] - 0s 143us/step - loss: 2499138.5631 - val_loss: 2710336.3228\n",
      "Epoch 81/100\n",
      "2584/2584 [==============================] - 0s 137us/step - loss: 2496169.9528 - val_loss: 2708890.2701\n",
      "Epoch 82/100\n",
      "2584/2584 [==============================] - 0s 110us/step - loss: 2493691.6985 - val_loss: 2708358.3522\n",
      "Epoch 83/100\n",
      "2584/2584 [==============================] - 0s 166us/step - loss: 2490802.3558 - val_loss: 2705271.3142\n",
      "Epoch 84/100\n",
      "2584/2584 [==============================] - 0s 144us/step - loss: 2487327.0708 - val_loss: 2704509.8034\n",
      "Epoch 85/100\n",
      "2584/2584 [==============================] - 0s 147us/step - loss: 2484639.5642 - val_loss: 2702940.7190\n",
      "Epoch 86/100\n",
      "2584/2584 [==============================] - 0s 107us/step - loss: 2481815.1242 - val_loss: 2702087.1207\n",
      "Epoch 87/100\n",
      "2584/2584 [==============================] - 0s 88us/step - loss: 2478841.0604 - val_loss: 2700742.6494\n",
      "Epoch 88/100\n",
      "2584/2584 [==============================] - 0s 87us/step - loss: 2475961.2039 - val_loss: 2698982.5944\n",
      "Epoch 89/100\n",
      "2584/2584 [==============================] - 0s 90us/step - loss: 2473242.3549 - val_loss: 2697594.6896\n",
      "Epoch 90/100\n",
      "2584/2584 [==============================] - 0s 103us/step - loss: 2470608.4290 - val_loss: 2696187.5882\n",
      "Epoch 91/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2467991.1807 - val_loss: 2695077.6858\n",
      "Epoch 92/100\n",
      "2584/2584 [==============================] - 0s 141us/step - loss: 2464852.2933 - val_loss: 2694611.8537\n",
      "Epoch 93/100\n",
      "2584/2584 [==============================] - 0s 157us/step - loss: 2462070.5224 - val_loss: 2692923.4977\n",
      "Epoch 94/100\n",
      "2584/2584 [==============================] - 0s 101us/step - loss: 2459393.2899 - val_loss: 2691761.4272\n",
      "Epoch 95/100\n",
      "2584/2584 [==============================] - 0s 88us/step - loss: 2456867.2728 - val_loss: 2690685.9249\n",
      "Epoch 96/100\n",
      "2584/2584 [==============================] - 0s 84us/step - loss: 2453843.9443 - val_loss: 2688321.3050\n",
      "Epoch 97/100\n",
      "2584/2584 [==============================] - 0s 146us/step - loss: 2451403.1308 - val_loss: 2687203.6772\n",
      "Epoch 98/100\n",
      "2584/2584 [==============================] - 0s 150us/step - loss: 2448373.1409 - val_loss: 2686197.6950\n",
      "Epoch 99/100\n",
      "2584/2584 [==============================] - 0s 140us/step - loss: 2446879.2744 - val_loss: 2686200.9667\n",
      "Epoch 100/100\n",
      "2584/2584 [==============================] - 0s 86us/step - loss: 2444539.0374 - val_loss: 2684316.7067\n",
      "1291/1291 [==============================] - 0s 32us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 2s 748us/step - loss: 2708981.9220 - val_loss: 2817140.6757\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 1s 459us/step - loss: 2455129.8723 - val_loss: 3055115.2245\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 1s 408us/step - loss: 2303107.3150 - val_loss: 2645104.8197\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 1s 315us/step - loss: 2168793.2808 - val_loss: 2759193.4961\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 1s 391us/step - loss: 2111692.9743 - val_loss: 2750978.4497\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 1s 297us/step - loss: 2042083.2664 - val_loss: 2855648.3437\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 1s 309us/step - loss: 1922445.2079 - val_loss: 3019566.9907\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 1s 313us/step - loss: 1892736.5632 - val_loss: 2922796.6683\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 1s 291us/step - loss: 1817852.5149 - val_loss: 2893428.5909\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 1s 317us/step - loss: 1909806.8173 - val_loss: 2917985.0232\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 1s 366us/step - loss: 1772981.2088 - val_loss: 3037272.2740\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 1s 358us/step - loss: 1721276.4001 - val_loss: 3437594.2260\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 1s 308us/step - loss: 1712233.3561 - val_loss: 2995315.8460\n",
      "1292/1292 [==============================] - 0s 71us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 2s 727us/step - loss: 2673269.3608 - val_loss: 2656172.6920\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 1s 473us/step - loss: 2291075.6302 - val_loss: 2631501.5666\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 1s 505us/step - loss: 2138410.2546 - val_loss: 2662727.1881\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 1s 559us/step - loss: 1976181.5419 - val_loss: 2878418.0062\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 1s 489us/step - loss: 1965239.5996 - val_loss: 2827497.1734\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 1s 492us/step - loss: 1880531.0097 - val_loss: 2888163.0697\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 1s 400us/step - loss: 1810644.3761 - val_loss: 3016876.0666\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 1s 409us/step - loss: 1784675.5947 - val_loss: 2775365.2159\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 1s 367us/step - loss: 1758577.8257 - val_loss: 2966106.8514\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 1s 438us/step - loss: 1778814.0885 - val_loss: 2870690.3173\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 1s 443us/step - loss: 1695541.6234 - val_loss: 2951828.5000\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 1s 428us/step - loss: 1689821.8590 - val_loss: 2803708.7748\n",
      "1292/1292 [==============================] - 0s 72us/step\n",
      "Train on 2584 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2584/2584 [==============================] - 2s 596us/step - loss: 2804979.5322 - val_loss: 2670077.1161\n",
      "Epoch 2/100\n",
      "2584/2584 [==============================] - 1s 419us/step - loss: 2464261.1556 - val_loss: 2647363.1935\n",
      "Epoch 3/100\n",
      "2584/2584 [==============================] - 1s 368us/step - loss: 2250315.1250 - val_loss: 2723532.4180\n",
      "Epoch 4/100\n",
      "2584/2584 [==============================] - 1s 352us/step - loss: 2111485.1343 - val_loss: 2794552.5998\n",
      "Epoch 5/100\n",
      "2584/2584 [==============================] - 1s 342us/step - loss: 2024433.1649 - val_loss: 3215087.9582\n",
      "Epoch 6/100\n",
      "2584/2584 [==============================] - 1s 377us/step - loss: 1834233.0561 - val_loss: 3114058.5372\n",
      "Epoch 7/100\n",
      "2584/2584 [==============================] - 1s 378us/step - loss: 1796465.4903 - val_loss: 2938895.5356\n",
      "Epoch 8/100\n",
      "2584/2584 [==============================] - 1s 342us/step - loss: 1784892.2895 - val_loss: 3313327.0836\n",
      "Epoch 9/100\n",
      "2584/2584 [==============================] - 1s 348us/step - loss: 1711104.8591 - val_loss: 3303942.0093\n",
      "Epoch 10/100\n",
      "2584/2584 [==============================] - 1s 345us/step - loss: 1695215.2235 - val_loss: 3209680.6803\n",
      "Epoch 11/100\n",
      "2584/2584 [==============================] - 1s 349us/step - loss: 1665767.1603 - val_loss: 3110842.4443\n",
      "Epoch 12/100\n",
      "2584/2584 [==============================] - 1s 351us/step - loss: 1633918.1248 - val_loss: 3325469.4195\n",
      "1291/1291 [==============================] - 0s 76us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 482us/step - loss: 2637244.7619 - val_loss: 2756482.0070\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 1s 293us/step - loss: 2388774.8796 - val_loss: 2638229.5511\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 1s 274us/step - loss: 2210670.5678 - val_loss: 2703736.2848\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 1s 267us/step - loss: 2110545.9540 - val_loss: 2855945.7655\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 1s 270us/step - loss: 2016241.2027 - val_loss: 2874457.8560\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 1s 264us/step - loss: 1924415.5151 - val_loss: 2756003.3560\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 1s 323us/step - loss: 1930308.3187 - val_loss: 3439411.8715\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 1s 281us/step - loss: 1900368.7274 - val_loss: 3136019.9466\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 1s 263us/step - loss: 1868180.0720 - val_loss: 2857760.5093\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 1s 263us/step - loss: 1728455.1070 - val_loss: 2939592.6223\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 1s 265us/step - loss: 1813811.6254 - val_loss: 3274828.1889\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 1s 277us/step - loss: 1687150.0851 - val_loss: 2867784.4954\n",
      "1292/1292 [==============================] - 0s 72us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 493us/step - loss: 2637289.0308 - val_loss: 2623380.3901\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 1s 311us/step - loss: 2309764.9700 - val_loss: 2674599.4853\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 1s 299us/step - loss: 2115554.6710 - val_loss: 2694117.9156\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 1s 278us/step - loss: 1995965.3525 - val_loss: 2891470.5023\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 1s 283us/step - loss: 1960686.9686 - val_loss: 2827392.1858\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 1s 282us/step - loss: 1896990.8057 - val_loss: 2766250.3947\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 1s 298us/step - loss: 1834421.1809 - val_loss: 2842164.9752\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 1s 288us/step - loss: 1833302.3856 - val_loss: 2690686.8274\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 1s 340us/step - loss: 1813163.2405 - val_loss: 2761117.6324\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 1s 301us/step - loss: 1774300.7163 - val_loss: 2876374.9087\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 1s 280us/step - loss: 1769993.4920 - val_loss: 2785839.0697\n",
      "1292/1292 [==============================] - 0s 71us/step\n",
      "Train on 2584 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2584/2584 [==============================] - 1s 482us/step - loss: 2722500.9698 - val_loss: 2653002.5364\n",
      "Epoch 2/100\n",
      "2584/2584 [==============================] - 1s 304us/step - loss: 2471777.0877 - val_loss: 2716636.0658\n",
      "Epoch 3/100\n",
      "2584/2584 [==============================] - 1s 401us/step - loss: 2311695.5673 - val_loss: 2794742.6842\n",
      "Epoch 4/100\n",
      "2584/2584 [==============================] - 1s 278us/step - loss: 2272203.9899 - val_loss: 2793067.4319\n",
      "Epoch 5/100\n",
      "2584/2584 [==============================] - 1s 276us/step - loss: 2078254.5263 - val_loss: 2919404.1393\n",
      "Epoch 6/100\n",
      "2584/2584 [==============================] - 1s 254us/step - loss: 2016317.4365 - val_loss: 3007963.1873\n",
      "Epoch 7/100\n",
      "2584/2584 [==============================] - 1s 267us/step - loss: 1916478.4826 - val_loss: 2812833.5797\n",
      "Epoch 8/100\n",
      "2584/2584 [==============================] - 1s 247us/step - loss: 1789218.7916 - val_loss: 3407613.1347\n",
      "Epoch 9/100\n",
      "2584/2584 [==============================] - 1s 251us/step - loss: 1781984.0977 - val_loss: 3090054.4241\n",
      "Epoch 10/100\n",
      "2584/2584 [==============================] - 1s 258us/step - loss: 1750210.0511 - val_loss: 3376934.7245\n",
      "Epoch 11/100\n",
      "2584/2584 [==============================] - 1s 260us/step - loss: 1758382.8735 - val_loss: 3168564.8421\n",
      "1291/1291 [==============================] - 0s 70us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 277us/step - loss: 3138164.4309 - val_loss: 2991166.2461\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 130us/step - loss: 2677459.9940 - val_loss: 2766054.2361\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2472275.2333 - val_loss: 2698396.3437\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 125us/step - loss: 2409469.7552 - val_loss: 2681663.2616\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 121us/step - loss: 2377835.2090 - val_loss: 2650365.0751\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 125us/step - loss: 2328434.9086 - val_loss: 2635486.3011\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 126us/step - loss: 2287078.4001 - val_loss: 2632510.9110\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 123us/step - loss: 2248020.8051 - val_loss: 2650869.0356\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 125us/step - loss: 2196914.8947 - val_loss: 2673074.5263\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 134us/step - loss: 2162494.5532 - val_loss: 2655063.6772\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 122us/step - loss: 2128369.9316 - val_loss: 2689146.5820\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 129us/step - loss: 2097737.8330 - val_loss: 2693253.9164\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 120us/step - loss: 2074721.1751 - val_loss: 2711182.6927\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 122us/step - loss: 2026313.0626 - val_loss: 2723998.7461\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 123us/step - loss: 1999602.7892 - val_loss: 2732555.6811\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 122us/step - loss: 1970680.1908 - val_loss: 2752770.0875\n",
      "Epoch 17/100\n",
      "2583/2583 [==============================] - 0s 123us/step - loss: 1939723.0484 - val_loss: 2781250.0611\n",
      "1292/1292 [==============================] - 0s 45us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 290us/step - loss: 3204123.4108 - val_loss: 2935358.3413\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 151us/step - loss: 2591748.9220 - val_loss: 2722287.5526\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 131us/step - loss: 2438072.6340 - val_loss: 2668073.6509\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 119us/step - loss: 2356268.1986 - val_loss: 2624932.2732\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 123us/step - loss: 2288815.0215 - val_loss: 2607931.2190\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 120us/step - loss: 2241445.2574 - val_loss: 2603861.3607\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 121us/step - loss: 2182909.5960 - val_loss: 2607813.7283\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 120us/step - loss: 2139607.3021 - val_loss: 2612742.5851\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 145us/step - loss: 2092544.7793 - val_loss: 2630101.6718\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 166us/step - loss: 2061944.3216 - val_loss: 2667725.6393\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 164us/step - loss: 2043669.9646 - val_loss: 2670209.5797\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 140us/step - loss: 1994960.9412 - val_loss: 2677406.1943\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 159us/step - loss: 1947950.2138 - val_loss: 2724146.9752\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 160us/step - loss: 1922016.1600 - val_loss: 2744006.9559\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 164us/step - loss: 1908213.2790 - val_loss: 2745092.3375\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 173us/step - loss: 1890781.6590 - val_loss: 2749577.9985\n",
      "1292/1292 [==============================] - 0s 41us/step\n",
      "Train on 2584 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2584/2584 [==============================] - 1s 314us/step - loss: 3217685.6449 - val_loss: 3024199.1502\n",
      "Epoch 2/100\n",
      "2584/2584 [==============================] - 0s 147us/step - loss: 2799510.0530 - val_loss: 2831261.5186\n",
      "Epoch 3/100\n",
      "2584/2584 [==============================] - 0s 142us/step - loss: 2608648.6041 - val_loss: 2719466.1053\n",
      "Epoch 4/100\n",
      "2584/2584 [==============================] - 0s 163us/step - loss: 2529887.3878 - val_loss: 2698299.4149\n",
      "Epoch 5/100\n",
      "2584/2584 [==============================] - 0s 169us/step - loss: 2465738.5085 - val_loss: 2641296.2005\n",
      "Epoch 6/100\n",
      "2584/2584 [==============================] - 0s 179us/step - loss: 2398780.3444 - val_loss: 2615866.5248\n",
      "Epoch 7/100\n",
      "2584/2584 [==============================] - 0s 159us/step - loss: 2348630.2933 - val_loss: 2627607.9420\n",
      "Epoch 8/100\n",
      "2584/2584 [==============================] - 0s 138us/step - loss: 2314342.4518 - val_loss: 2610471.9211\n",
      "Epoch 9/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2261110.5178 - val_loss: 2681939.2322\n",
      "Epoch 10/100\n",
      "2584/2584 [==============================] - 0s 124us/step - loss: 2214278.7759 - val_loss: 2638553.1440\n",
      "Epoch 11/100\n",
      "2584/2584 [==============================] - 0s 142us/step - loss: 2190828.3858 - val_loss: 2651131.4257\n",
      "Epoch 12/100\n",
      "2584/2584 [==============================] - 0s 127us/step - loss: 2139912.1209 - val_loss: 2676312.9845\n",
      "Epoch 13/100\n",
      "2584/2584 [==============================] - 0s 153us/step - loss: 2096233.4450 - val_loss: 2713736.3437\n",
      "Epoch 14/100\n",
      "2584/2584 [==============================] - 0s 159us/step - loss: 2076874.1983 - val_loss: 2729637.1440\n",
      "Epoch 15/100\n",
      "2584/2584 [==============================] - 0s 148us/step - loss: 2051501.8537 - val_loss: 2770980.8808\n",
      "Epoch 16/100\n",
      "2584/2584 [==============================] - 0s 167us/step - loss: 2010322.9725 - val_loss: 2756567.0681\n",
      "Epoch 17/100\n",
      "2584/2584 [==============================] - 0s 160us/step - loss: 2006744.2485 - val_loss: 2772541.6122\n",
      "Epoch 18/100\n",
      "2584/2584 [==============================] - 0s 145us/step - loss: 1968401.6115 - val_loss: 2829948.3460\n",
      "1291/1291 [==============================] - 0s 52us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 577us/step - loss: 2810708.5938 - val_loss: 2709465.5132\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 1s 358us/step - loss: 2430351.9669 - val_loss: 2643914.2314\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 1s 350us/step - loss: 2342866.1306 - val_loss: 2692793.1981\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 1s 370us/step - loss: 2259434.3427 - val_loss: 2688883.3452\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 1s 318us/step - loss: 2155271.8716 - val_loss: 2716929.9876\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 1s 339us/step - loss: 2061046.6898 - val_loss: 2733391.8127\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 1s 357us/step - loss: 2001775.8285 - val_loss: 2726235.9133\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 1s 326us/step - loss: 1946827.7506 - val_loss: 2801907.9628\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 1s 345us/step - loss: 1892271.1826 - val_loss: 2756943.3034\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 1s 360us/step - loss: 1818491.5972 - val_loss: 2855111.6525\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 1s 348us/step - loss: 1810478.0883 - val_loss: 2841606.3649\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 1s 387us/step - loss: 1764810.3864 - val_loss: 3021402.0875\n",
      "1292/1292 [==============================] - 0s 69us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 567us/step - loss: 2806330.7887 - val_loss: 2720134.1757\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 1s 372us/step - loss: 2395343.3439 - val_loss: 2616453.2036\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 1s 367us/step - loss: 2233416.6185 - val_loss: 2670828.5759\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 1s 334us/step - loss: 2118578.5353 - val_loss: 2712919.8978\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 1s 404us/step - loss: 2049295.0383 - val_loss: 2724413.4056\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 1s 418us/step - loss: 1974377.9269 - val_loss: 2797125.2910\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 1s 372us/step - loss: 1868367.9045 - val_loss: 2773109.8971\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 1s 380us/step - loss: 1830087.2911 - val_loss: 2844560.4327\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 1s 416us/step - loss: 1782216.5854 - val_loss: 2993234.4613\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 1s 399us/step - loss: 1739453.1152 - val_loss: 2940410.4768\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 1s 386us/step - loss: 1710477.9400 - val_loss: 2886674.8885\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 1s 387us/step - loss: 1678814.0073 - val_loss: 2898471.6362\n",
      "1292/1292 [==============================] - 0s 82us/step\n",
      "Train on 2584 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2584/2584 [==============================] - 1s 465us/step - loss: 2884133.8297 - val_loss: 2682956.7283\n",
      "Epoch 2/100\n",
      "2584/2584 [==============================] - 1s 295us/step - loss: 2492510.2132 - val_loss: 2627092.6904\n",
      "Epoch 3/100\n",
      "2584/2584 [==============================] - 1s 286us/step - loss: 2325246.3367 - val_loss: 2627026.5557\n",
      "Epoch 4/100\n",
      "2584/2584 [==============================] - 1s 291us/step - loss: 2272051.1668 - val_loss: 2754137.0217\n",
      "Epoch 5/100\n",
      "2584/2584 [==============================] - 1s 267us/step - loss: 2174411.3615 - val_loss: 2776669.9536\n",
      "Epoch 6/100\n",
      "2584/2584 [==============================] - 1s 322us/step - loss: 2102571.5906 - val_loss: 2886017.3421\n",
      "Epoch 7/100\n",
      "2584/2584 [==============================] - 1s 317us/step - loss: 2022351.8607 - val_loss: 2950384.7245\n",
      "Epoch 8/100\n",
      "2584/2584 [==============================] - 1s 327us/step - loss: 1900847.6854 - val_loss: 3041495.6440\n",
      "Epoch 9/100\n",
      "2584/2584 [==============================] - 1s 308us/step - loss: 1867169.0863 - val_loss: 3126054.6517\n",
      "Epoch 10/100\n",
      "2584/2584 [==============================] - 1s 318us/step - loss: 1788599.4040 - val_loss: 3214879.4118\n",
      "Epoch 11/100\n",
      "2584/2584 [==============================] - 1s 331us/step - loss: 1763350.2798 - val_loss: 3205106.4489\n",
      "Epoch 12/100\n",
      "2584/2584 [==============================] - 1s 310us/step - loss: 1675944.5358 - val_loss: 3471105.2074\n",
      "Epoch 13/100\n",
      "2584/2584 [==============================] - 1s 315us/step - loss: 1643319.3781 - val_loss: 3209587.5232\n",
      "1291/1291 [==============================] - 0s 68us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 306us/step - loss: 3541087.7603 - val_loss: 3776874.8560\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 142us/step - loss: 3428344.7493 - val_loss: 3563216.1486\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 140us/step - loss: 3116683.3206 - val_loss: 3141223.7577\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 144us/step - loss: 2843170.7113 - val_loss: 3001308.2864\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2775962.2100 - val_loss: 2953021.9574\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2722935.0594 - val_loss: 2900687.1045\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 132us/step - loss: 2660266.4734 - val_loss: 2846994.7546\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 132us/step - loss: 2598881.0877 - val_loss: 2798958.2415\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 133us/step - loss: 2546662.0991 - val_loss: 2765097.5557\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 134us/step - loss: 2509688.6184 - val_loss: 2744433.4621\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 143us/step - loss: 2484189.4674 - val_loss: 2731713.9296\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 133us/step - loss: 2469508.6971 - val_loss: 2724495.2128\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 133us/step - loss: 2457591.8074 - val_loss: 2721482.0232\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 132us/step - loss: 2448935.4377 - val_loss: 2715709.0913\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 133us/step - loss: 2440256.4158 - val_loss: 2710454.8638\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 134us/step - loss: 2431846.1370 - val_loss: 2706856.1076\n",
      "Epoch 17/100\n",
      "2583/2583 [==============================] - 0s 134us/step - loss: 2424069.3543 - val_loss: 2703448.3568\n",
      "Epoch 18/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2415277.0363 - val_loss: 2698876.6347\n",
      "Epoch 19/100\n",
      "2583/2583 [==============================] - 0s 155us/step - loss: 2408586.0863 - val_loss: 2693737.6471\n",
      "Epoch 20/100\n",
      "2583/2583 [==============================] - 0s 169us/step - loss: 2400104.1012 - val_loss: 2691267.8235\n",
      "Epoch 21/100\n",
      "2583/2583 [==============================] - 0s 182us/step - loss: 2391926.2634 - val_loss: 2686199.3839\n",
      "Epoch 22/100\n",
      "2583/2583 [==============================] - 0s 159us/step - loss: 2384000.7520 - val_loss: 2680988.7229\n",
      "Epoch 23/100\n",
      "2583/2583 [==============================] - 0s 172us/step - loss: 2378935.3848 - val_loss: 2678020.9574\n",
      "Epoch 24/100\n",
      "2583/2583 [==============================] - 0s 147us/step - loss: 2370608.6113 - val_loss: 2676046.1974\n",
      "Epoch 25/100\n",
      "2583/2583 [==============================] - 0s 143us/step - loss: 2362554.0688 - val_loss: 2672729.1215\n",
      "Epoch 26/100\n",
      "2583/2583 [==============================] - 0s 153us/step - loss: 2354684.9063 - val_loss: 2668627.2593\n",
      "Epoch 27/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2346133.1484 - val_loss: 2665563.6308\n",
      "Epoch 28/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2338870.0747 - val_loss: 2664767.4923\n",
      "Epoch 29/100\n",
      "2583/2583 [==============================] - 0s 139us/step - loss: 2329464.3569 - val_loss: 2659991.4946\n",
      "Epoch 30/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2322809.5140 - val_loss: 2658872.0697\n",
      "Epoch 31/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2313794.4430 - val_loss: 2653821.6207\n",
      "Epoch 32/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2306625.4780 - val_loss: 2653882.2028\n",
      "Epoch 33/100\n",
      "2583/2583 [==============================] - 0s 139us/step - loss: 2300167.3624 - val_loss: 2653012.3127\n",
      "Epoch 34/100\n",
      "2583/2583 [==============================] - 0s 139us/step - loss: 2291528.6813 - val_loss: 2647835.0681\n",
      "Epoch 35/100\n",
      "2583/2583 [==============================] - 0s 152us/step - loss: 2282941.6215 - val_loss: 2647975.4729\n",
      "Epoch 36/100\n",
      "2583/2583 [==============================] - 0s 139us/step - loss: 2273334.2651 - val_loss: 2645296.9892\n",
      "Epoch 37/100\n",
      "2583/2583 [==============================] - 0s 143us/step - loss: 2265668.1871 - val_loss: 2643222.6076\n",
      "Epoch 38/100\n",
      "2583/2583 [==============================] - 0s 139us/step - loss: 2257945.3732 - val_loss: 2644024.9892\n",
      "Epoch 39/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2247888.4753 - val_loss: 2644506.7307\n",
      "Epoch 40/100\n",
      "2583/2583 [==============================] - 0s 140us/step - loss: 2241485.0956 - val_loss: 2643355.7810\n",
      "Epoch 41/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2236012.8026 - val_loss: 2642817.7276\n",
      "Epoch 42/100\n",
      "2583/2583 [==============================] - 0s 159us/step - loss: 2227216.6429 - val_loss: 2642890.6316\n",
      "Epoch 43/100\n",
      "2583/2583 [==============================] - 0s 139us/step - loss: 2218146.0459 - val_loss: 2644044.4056\n",
      "Epoch 44/100\n",
      "2583/2583 [==============================] - 0s 140us/step - loss: 2207756.4366 - val_loss: 2643862.1757\n",
      "Epoch 45/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2199683.7674 - val_loss: 2645823.8475\n",
      "Epoch 46/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2192233.5949 - val_loss: 2649012.8514\n",
      "Epoch 47/100\n",
      "2583/2583 [==============================] - 0s 139us/step - loss: 2187385.7889 - val_loss: 2648272.3793\n",
      "Epoch 48/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2176838.1227 - val_loss: 2648268.7523\n",
      "Epoch 49/100\n",
      "2583/2583 [==============================] - 0s 147us/step - loss: 2169087.9654 - val_loss: 2650031.6486\n",
      "Epoch 50/100\n",
      "2583/2583 [==============================] - 0s 190us/step - loss: 2162782.3972 - val_loss: 2653957.4636\n",
      "Epoch 51/100\n",
      "2583/2583 [==============================] - 0s 140us/step - loss: 2152437.0519 - val_loss: 2654526.9822\n",
      "1292/1292 [==============================] - 0s 48us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 321us/step - loss: 3565774.1437 - val_loss: 3797164.9195\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 0s 169us/step - loss: 3488259.6154 - val_loss: 3660190.7802\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 0s 146us/step - loss: 3272901.9622 - val_loss: 3314721.1053\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 0s 158us/step - loss: 2941312.9363 - val_loss: 3049984.0859\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 0s 143us/step - loss: 2830160.0190 - val_loss: 3022119.6393\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 151us/step - loss: 2807880.4798 - val_loss: 3002448.9652\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2783145.4993 - val_loss: 2977638.8676\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2755166.5777 - val_loss: 2950165.8259\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2720174.3758 - val_loss: 2915122.6347\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2676565.9087 - val_loss: 2877250.5433\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2630537.2535 - val_loss: 2836232.6904\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 134us/step - loss: 2587102.3091 - val_loss: 2802851.7012\n",
      "Epoch 13/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2542650.3014 - val_loss: 2766005.6966\n",
      "Epoch 14/100\n",
      "2583/2583 [==============================] - 0s 149us/step - loss: 2509312.1622 - val_loss: 2742594.7368\n",
      "Epoch 15/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2481837.7691 - val_loss: 2725712.2438\n",
      "Epoch 16/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2458981.3273 - val_loss: 2712263.2260\n",
      "Epoch 17/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2441751.4439 - val_loss: 2704348.3506\n",
      "Epoch 18/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2425268.1072 - val_loss: 2697814.9311\n",
      "Epoch 19/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2413304.5370 - val_loss: 2689027.7779\n",
      "Epoch 20/100\n",
      "2583/2583 [==============================] - 0s 146us/step - loss: 2401061.8502 - val_loss: 2680266.8847\n",
      "Epoch 21/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2385563.4567 - val_loss: 2677269.5875\n",
      "Epoch 22/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2378609.0272 - val_loss: 2668627.3955\n",
      "Epoch 23/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2362884.5887 - val_loss: 2662035.2314\n",
      "Epoch 24/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2352296.2650 - val_loss: 2656212.6130\n",
      "Epoch 25/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2337780.6544 - val_loss: 2649103.5704\n",
      "Epoch 26/100\n",
      "2583/2583 [==============================] - 0s 150us/step - loss: 2325108.8148 - val_loss: 2645076.0163\n",
      "Epoch 27/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2317220.9535 - val_loss: 2639357.9814\n",
      "Epoch 28/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2303954.4689 - val_loss: 2632597.2175\n",
      "Epoch 29/100\n",
      "2583/2583 [==============================] - 0s 146us/step - loss: 2292529.5515 - val_loss: 2628387.4961\n",
      "Epoch 30/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2282479.5491 - val_loss: 2627324.2608\n",
      "Epoch 31/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2269304.2232 - val_loss: 2621798.2554\n",
      "Epoch 32/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2263052.3379 - val_loss: 2618877.4211\n",
      "Epoch 33/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2245891.6644 - val_loss: 2617515.5341\n",
      "Epoch 34/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2238902.5708 - val_loss: 2611779.2492\n",
      "Epoch 35/100\n",
      "2583/2583 [==============================] - 0s 149us/step - loss: 2225359.7045 - val_loss: 2611517.3715\n",
      "Epoch 36/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2216623.5737 - val_loss: 2607364.6966\n",
      "Epoch 37/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2204583.0302 - val_loss: 2605427.3831\n",
      "Epoch 38/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2195824.0615 - val_loss: 2604356.3220\n",
      "Epoch 39/100\n",
      "2583/2583 [==============================] - 0s 147us/step - loss: 2185666.8850 - val_loss: 2602813.3646\n",
      "Epoch 40/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2175407.9445 - val_loss: 2602370.9195\n",
      "Epoch 41/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2167174.0159 - val_loss: 2602253.2175\n",
      "Epoch 42/100\n",
      "2583/2583 [==============================] - 0s 141us/step - loss: 2158279.5409 - val_loss: 2602932.0271\n",
      "Epoch 43/100\n",
      "2583/2583 [==============================] - 0s 149us/step - loss: 2146929.3720 - val_loss: 2601360.1989\n",
      "Epoch 44/100\n",
      "2583/2583 [==============================] - 0s 137us/step - loss: 2136704.6777 - val_loss: 2606227.7175\n",
      "Epoch 45/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2131374.6274 - val_loss: 2604182.4234\n",
      "Epoch 46/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2123397.5545 - val_loss: 2603598.5209\n",
      "Epoch 47/100\n",
      "2583/2583 [==============================] - 0s 135us/step - loss: 2111684.9963 - val_loss: 2605318.9768\n",
      "Epoch 48/100\n",
      "2583/2583 [==============================] - 0s 149us/step - loss: 2105053.5809 - val_loss: 2606377.3591\n",
      "Epoch 49/100\n",
      "2583/2583 [==============================] - 0s 139us/step - loss: 2098624.4151 - val_loss: 2606167.6834\n",
      "Epoch 50/100\n",
      "2583/2583 [==============================] - 0s 136us/step - loss: 2090179.1005 - val_loss: 2610712.0341\n",
      "Epoch 51/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2081524.7290 - val_loss: 2609987.3135\n",
      "Epoch 52/100\n",
      "2583/2583 [==============================] - 0s 152us/step - loss: 2071352.9767 - val_loss: 2612899.6927\n",
      "Epoch 53/100\n",
      "2583/2583 [==============================] - 0s 138us/step - loss: 2062707.0451 - val_loss: 2621329.2531\n",
      "1292/1292 [==============================] - 0s 48us/step\n",
      "Train on 2584 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2584/2584 [==============================] - 1s 321us/step - loss: 3612234.7378 - val_loss: 3781612.5805\n",
      "Epoch 2/100\n",
      "2584/2584 [==============================] - 0s 149us/step - loss: 3514455.9938 - val_loss: 3588501.2601\n",
      "Epoch 3/100\n",
      "2584/2584 [==============================] - 0s 137us/step - loss: 3245106.9644 - val_loss: 3213137.8947\n",
      "Epoch 4/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2961591.0422 - val_loss: 3036438.9280\n",
      "Epoch 5/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2902218.4601 - val_loss: 3010369.7028\n",
      "Epoch 6/100\n",
      "2584/2584 [==============================] - 0s 137us/step - loss: 2874853.7368 - val_loss: 2983657.2183\n",
      "Epoch 7/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2839127.1573 - val_loss: 2946325.1060\n",
      "Epoch 8/100\n",
      "2584/2584 [==============================] - 0s 146us/step - loss: 2795929.5182 - val_loss: 2907840.3251\n",
      "Epoch 9/100\n",
      "2584/2584 [==============================] - 0s 137us/step - loss: 2747733.1974 - val_loss: 2859599.9683\n",
      "Epoch 10/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2698542.5530 - val_loss: 2817602.7763\n",
      "Epoch 11/100\n",
      "2584/2584 [==============================] - 0s 138us/step - loss: 2650923.9156 - val_loss: 2779824.7430\n",
      "Epoch 12/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2612407.7663 - val_loss: 2751068.4094\n",
      "Epoch 13/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2580857.7643 - val_loss: 2731215.3204\n",
      "Epoch 14/100\n",
      "2584/2584 [==============================] - 0s 147us/step - loss: 2560087.8824 - val_loss: 2715909.3444\n",
      "Epoch 15/100\n",
      "2584/2584 [==============================] - 0s 141us/step - loss: 2541658.6029 - val_loss: 2706324.8785\n",
      "Epoch 16/100\n",
      "2584/2584 [==============================] - 0s 146us/step - loss: 2529551.1314 - val_loss: 2697705.8460\n",
      "Epoch 17/100\n",
      "2584/2584 [==============================] - 0s 137us/step - loss: 2516543.5534 - val_loss: 2691051.5302\n",
      "Epoch 18/100\n",
      "2584/2584 [==============================] - 0s 134us/step - loss: 2504447.3108 - val_loss: 2680919.6447\n",
      "Epoch 19/100\n",
      "2584/2584 [==============================] - 0s 147us/step - loss: 2492883.4979 - val_loss: 2678407.9481\n",
      "Epoch 20/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2485509.3646 - val_loss: 2670654.3901\n",
      "Epoch 21/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2473302.2043 - val_loss: 2666349.4412\n",
      "Epoch 22/100\n",
      "2584/2584 [==============================] - 0s 137us/step - loss: 2462425.8847 - val_loss: 2657041.9466\n",
      "Epoch 23/100\n",
      "2584/2584 [==============================] - 0s 146us/step - loss: 2453788.8117 - val_loss: 2648870.5395\n",
      "Epoch 24/100\n",
      "2584/2584 [==============================] - 0s 139us/step - loss: 2441523.3459 - val_loss: 2645358.5310\n",
      "Epoch 25/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2431973.0848 - val_loss: 2640733.3963\n",
      "Epoch 26/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2421500.6741 - val_loss: 2633573.4652\n",
      "Epoch 27/100\n",
      "2584/2584 [==============================] - 0s 137us/step - loss: 2411722.6540 - val_loss: 2628716.7771\n",
      "Epoch 28/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2400254.0329 - val_loss: 2624851.2933\n",
      "Epoch 29/100\n",
      "2584/2584 [==============================] - 0s 144us/step - loss: 2390652.7868 - val_loss: 2620619.1308\n",
      "Epoch 30/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2380480.4404 - val_loss: 2616743.0271\n",
      "Epoch 31/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2369910.6943 - val_loss: 2615920.4420\n",
      "Epoch 32/100\n",
      "2584/2584 [==============================] - 0s 138us/step - loss: 2360221.5704 - val_loss: 2610117.9280\n",
      "Epoch 33/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2351139.7337 - val_loss: 2606946.9536\n",
      "Epoch 34/100\n",
      "2584/2584 [==============================] - 0s 137us/step - loss: 2341625.2535 - val_loss: 2605528.3638\n",
      "Epoch 35/100\n",
      "2584/2584 [==============================] - 0s 147us/step - loss: 2332465.6678 - val_loss: 2603327.4923\n",
      "Epoch 36/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2324219.9714 - val_loss: 2601249.8560\n",
      "Epoch 37/100\n",
      "2584/2584 [==============================] - 0s 135us/step - loss: 2315472.0283 - val_loss: 2600198.3940\n",
      "Epoch 38/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2306035.1869 - val_loss: 2599737.5170\n",
      "Epoch 39/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2297227.8634 - val_loss: 2599277.1509\n",
      "Epoch 40/100\n",
      "2584/2584 [==============================] - 0s 149us/step - loss: 2286494.3146 - val_loss: 2599563.9884\n",
      "Epoch 41/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2277984.1676 - val_loss: 2598569.5797\n",
      "Epoch 42/100\n",
      "2584/2584 [==============================] - 0s 137us/step - loss: 2270197.8727 - val_loss: 2599215.8034\n",
      "Epoch 43/100\n",
      "2584/2584 [==============================] - 0s 148us/step - loss: 2262022.1312 - val_loss: 2601583.2647\n",
      "Epoch 44/100\n",
      "2584/2584 [==============================] - 0s 137us/step - loss: 2252398.6320 - val_loss: 2601481.5813\n",
      "Epoch 45/100\n",
      "2584/2584 [==============================] - 0s 146us/step - loss: 2246249.8320 - val_loss: 2601399.7709\n",
      "Epoch 46/100\n",
      "2584/2584 [==============================] - 0s 136us/step - loss: 2236031.2024 - val_loss: 2601427.9365\n",
      "Epoch 47/100\n",
      "2584/2584 [==============================] - 0s 138us/step - loss: 2228915.5035 - val_loss: 2602215.6509\n",
      "Epoch 48/100\n",
      "2584/2584 [==============================] - 0s 134us/step - loss: 2218120.3491 - val_loss: 2605357.6981\n",
      "Epoch 49/100\n",
      "2584/2584 [==============================] - 0s 158us/step - loss: 2208500.3301 - val_loss: 2612863.5341\n",
      "Epoch 50/100\n",
      "2584/2584 [==============================] - 0s 151us/step - loss: 2204008.2779 - val_loss: 2608692.4187\n",
      "Epoch 51/100\n",
      "2584/2584 [==============================] - 0s 146us/step - loss: 2194900.8237 - val_loss: 2611515.5743\n",
      "1291/1291 [==============================] - 0s 64us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 449us/step - loss: 2690164.6410 - val_loss: 2690589.1896\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 1s 269us/step - loss: 2374224.8318 - val_loss: 2684453.1223\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 1s 219us/step - loss: 2264156.3433 - val_loss: 2695987.8738\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 1s 249us/step - loss: 2165791.7325 - val_loss: 2685472.7345\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 1s 225us/step - loss: 2052736.5191 - val_loss: 2803283.4234\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 0s 176us/step - loss: 2005727.4397 - val_loss: 2755142.7554\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 172us/step - loss: 1948515.7575 - val_loss: 2791251.1947\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 1s 207us/step - loss: 1881653.2580 - val_loss: 2948107.3460\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 185us/step - loss: 1839144.1276 - val_loss: 2880640.4319\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 175us/step - loss: 1764016.1706 - val_loss: 3030786.1525\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 177us/step - loss: 1823388.0499 - val_loss: 2919555.6935\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 188us/step - loss: 1791194.9502 - val_loss: 3091535.8003\n",
      "1292/1292 [==============================] - 0s 55us/step\n",
      "Train on 2583 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2583/2583 [==============================] - 1s 390us/step - loss: 2683204.3548 - val_loss: 2675530.2376\n",
      "Epoch 2/100\n",
      "2583/2583 [==============================] - 1s 282us/step - loss: 2366579.4244 - val_loss: 2598954.3831\n",
      "Epoch 3/100\n",
      "2583/2583 [==============================] - 1s 290us/step - loss: 2201492.5559 - val_loss: 2634584.2895\n",
      "Epoch 4/100\n",
      "2583/2583 [==============================] - 1s 240us/step - loss: 2084623.6433 - val_loss: 2732101.0712\n",
      "Epoch 5/100\n",
      "2583/2583 [==============================] - 1s 213us/step - loss: 2028007.2958 - val_loss: 2777557.1169\n",
      "Epoch 6/100\n",
      "2583/2583 [==============================] - 1s 222us/step - loss: 1952645.9698 - val_loss: 2756407.8529\n",
      "Epoch 7/100\n",
      "2583/2583 [==============================] - 0s 193us/step - loss: 1913927.6454 - val_loss: 2862769.8297\n",
      "Epoch 8/100\n",
      "2583/2583 [==============================] - 0s 179us/step - loss: 1832562.2801 - val_loss: 2881397.2105\n",
      "Epoch 9/100\n",
      "2583/2583 [==============================] - 0s 178us/step - loss: 1803419.4153 - val_loss: 2935546.6850\n",
      "Epoch 10/100\n",
      "2583/2583 [==============================] - 0s 189us/step - loss: 1766965.9345 - val_loss: 3082614.0046\n",
      "Epoch 11/100\n",
      "2583/2583 [==============================] - 0s 175us/step - loss: 1717181.4040 - val_loss: 2967363.4025\n",
      "Epoch 12/100\n",
      "2583/2583 [==============================] - 0s 174us/step - loss: 1712301.5627 - val_loss: 3043523.3460\n",
      "1292/1292 [==============================] - 0s 59us/step\n",
      "Train on 2584 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "2584/2584 [==============================] - 1s 365us/step - loss: 2741469.5379 - val_loss: 2684100.7376\n",
      "Epoch 2/100\n",
      "2584/2584 [==============================] - 1s 273us/step - loss: 2506899.8769 - val_loss: 2639430.5495\n",
      "Epoch 3/100\n",
      "2584/2584 [==============================] - 1s 304us/step - loss: 2336011.0855 - val_loss: 2791959.0186\n",
      "Epoch 4/100\n",
      "2584/2584 [==============================] - 1s 219us/step - loss: 2235013.8315 - val_loss: 2749043.0317\n",
      "Epoch 5/100\n",
      "2584/2584 [==============================] - 0s 188us/step - loss: 2134799.8765 - val_loss: 2792123.4033\n",
      "Epoch 6/100\n",
      "2584/2584 [==============================] - 1s 198us/step - loss: 2075912.7693 - val_loss: 2877592.1153\n",
      "Epoch 7/100\n",
      "2584/2584 [==============================] - 0s 189us/step - loss: 2009040.1022 - val_loss: 2929521.7307\n",
      "Epoch 8/100\n",
      "2584/2584 [==============================] - 0s 190us/step - loss: 1925192.2577 - val_loss: 3002437.6772\n",
      "Epoch 9/100\n",
      "2584/2584 [==============================] - 0s 191us/step - loss: 1892239.4371 - val_loss: 3046945.5325\n",
      "Epoch 10/100\n",
      "2584/2584 [==============================] - 1s 207us/step - loss: 1801404.3378 - val_loss: 3183128.1649\n",
      "Epoch 11/100\n",
      "2584/2584 [==============================] - 1s 256us/step - loss: 1747984.6533 - val_loss: 3134586.7724\n",
      "Epoch 12/100\n",
      "2584/2584 [==============================] - 1s 236us/step - loss: 1749257.3229 - val_loss: 3214230.7817\n",
      "1291/1291 [==============================] - 0s 69us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gskgagan/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/model_selection/_search.py:793: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3875 samples, validate on 1292 samples\n",
      "Epoch 1/100\n",
      "3875/3875 [==============================] - 1s 272us/step - loss: 3516557.9159 - val_loss: 3632021.2353\n",
      "Epoch 2/100\n",
      "3875/3875 [==============================] - 1s 166us/step - loss: 3164743.1124 - val_loss: 3106705.3545\n",
      "Epoch 3/100\n",
      "3875/3875 [==============================] - 1s 159us/step - loss: 2867857.8161 - val_loss: 3018568.9373\n",
      "Epoch 4/100\n",
      "3875/3875 [==============================] - 1s 145us/step - loss: 2822880.1626 - val_loss: 2978589.4420\n",
      "Epoch 5/100\n",
      "3875/3875 [==============================] - 1s 155us/step - loss: 2770469.8108 - val_loss: 2922002.2376\n",
      "Epoch 6/100\n",
      "3875/3875 [==============================] - 1s 168us/step - loss: 2706947.4685 - val_loss: 2863372.8197\n",
      "Epoch 7/100\n",
      "3875/3875 [==============================] - 1s 171us/step - loss: 2643327.5403 - val_loss: 2804531.4420\n",
      "Epoch 8/100\n",
      "3875/3875 [==============================] - 1s 154us/step - loss: 2584162.5103 - val_loss: 2760346.4899\n",
      "Epoch 9/100\n",
      "3875/3875 [==============================] - 1s 164us/step - loss: 2541219.7917 - val_loss: 2735142.9961\n",
      "Epoch 10/100\n",
      "3875/3875 [==============================] - 1s 156us/step - loss: 2514131.7404 - val_loss: 2717619.9187\n",
      "Epoch 11/100\n",
      "3875/3875 [==============================] - 1s 172us/step - loss: 2496655.5008 - val_loss: 2709804.8274\n",
      "Epoch 12/100\n",
      "3875/3875 [==============================] - 1s 168us/step - loss: 2484802.8440 - val_loss: 2698592.0279\n",
      "Epoch 13/100\n",
      "3875/3875 [==============================] - 1s 172us/step - loss: 2472899.5922 - val_loss: 2689016.3878\n",
      "Epoch 14/100\n",
      "3875/3875 [==============================] - 1s 164us/step - loss: 2460814.0484 - val_loss: 2682416.7492\n",
      "Epoch 15/100\n",
      "3875/3875 [==============================] - 1s 149us/step - loss: 2451602.1876 - val_loss: 2676269.9420\n",
      "Epoch 16/100\n",
      "3875/3875 [==============================] - 0s 129us/step - loss: 2442732.4213 - val_loss: 2669016.4861\n",
      "Epoch 17/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2431746.1303 - val_loss: 2665710.1819\n",
      "Epoch 18/100\n",
      "3875/3875 [==============================] - 0s 129us/step - loss: 2425020.5196 - val_loss: 2658570.0302\n",
      "Epoch 19/100\n",
      "3875/3875 [==============================] - 0s 128us/step - loss: 2415223.0163 - val_loss: 2650847.8305\n",
      "Epoch 20/100\n",
      "3875/3875 [==============================] - 0s 128us/step - loss: 2406146.4518 - val_loss: 2644024.3483\n",
      "Epoch 21/100\n",
      "3875/3875 [==============================] - 1s 132us/step - loss: 2396542.4610 - val_loss: 2638721.7639\n",
      "Epoch 22/100\n",
      "3875/3875 [==============================] - 1s 135us/step - loss: 2387954.6248 - val_loss: 2634235.6223\n",
      "Epoch 23/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2377894.2869 - val_loss: 2627900.2949\n",
      "Epoch 24/100\n",
      "3875/3875 [==============================] - 1s 131us/step - loss: 2369212.6774 - val_loss: 2624853.3421\n",
      "Epoch 25/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2360995.6449 - val_loss: 2619067.1331\n",
      "Epoch 26/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2351463.7929 - val_loss: 2614998.8483\n",
      "Epoch 27/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2342646.2378 - val_loss: 2611016.3127\n",
      "Epoch 28/100\n",
      "3875/3875 [==============================] - 1s 129us/step - loss: 2333236.9094 - val_loss: 2608124.3220\n",
      "Epoch 29/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2325292.2600 - val_loss: 2604771.2307\n",
      "Epoch 30/100\n",
      "3875/3875 [==============================] - 1s 129us/step - loss: 2316807.0514 - val_loss: 2601599.5209\n",
      "Epoch 31/100\n",
      "3875/3875 [==============================] - 1s 132us/step - loss: 2306333.0628 - val_loss: 2603535.1169\n",
      "Epoch 32/100\n",
      "3875/3875 [==============================] - 1s 140us/step - loss: 2298756.5642 - val_loss: 2598937.7980\n",
      "Epoch 33/100\n",
      "3875/3875 [==============================] - 1s 129us/step - loss: 2290644.2131 - val_loss: 2597987.1122\n",
      "Epoch 34/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2283312.6190 - val_loss: 2596928.0248\n",
      "Epoch 35/100\n",
      "3875/3875 [==============================] - 1s 132us/step - loss: 2276390.9050 - val_loss: 2596475.7957\n",
      "Epoch 36/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2265937.2173 - val_loss: 2596826.6231\n",
      "Epoch 37/100\n",
      "3875/3875 [==============================] - 0s 128us/step - loss: 2260418.0992 - val_loss: 2596519.6827\n",
      "Epoch 38/100\n",
      "3875/3875 [==============================] - 1s 131us/step - loss: 2250167.2813 - val_loss: 2596526.3622\n",
      "Epoch 39/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2242547.1040 - val_loss: 2600004.3050\n",
      "Epoch 40/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2238087.8107 - val_loss: 2598772.9156\n",
      "Epoch 41/100\n",
      "3875/3875 [==============================] - 1s 130us/step - loss: 2229716.8748 - val_loss: 2600400.6447\n",
      "Epoch 42/100\n",
      "3875/3875 [==============================] - 1s 147us/step - loss: 2220669.7772 - val_loss: 2602657.5356\n",
      "Epoch 43/100\n",
      "3875/3875 [==============================] - 1s 146us/step - loss: 2218591.4493 - val_loss: 2602485.0851\n",
      "Epoch 44/100\n",
      "3875/3875 [==============================] - 1s 142us/step - loss: 2210026.1346 - val_loss: 2605214.3406\n",
      "Epoch 45/100\n",
      "3875/3875 [==============================] - 1s 131us/step - loss: 2203926.5599 - val_loss: 2604632.7577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7ff3dd5cfa10>,\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'connect_input': [True, False],\n",
       "                                        'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7ff3dd5cfa50>,\n",
       "                                        'loss_fn': ['mse'],\n",
       "                                        'num_layers': [1, 2, 3, 4],\n",
       "                                        'num_neurons': array([ 1,  6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81,\n",
       "       86, 91, 96])},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing Randomized Parameter Search on Neural Network\n",
    "input_shape, output_shape = train_t[0].shape[1], train_t[1].shape[1]\n",
    "\n",
    "model_reg = keras.wrappers.scikit_learn.KerasRegressor(hyper_neural_network,\n",
    "                input_shape=input_shape, output_shape=output_shape, \n",
    "                num_layers=3, num_neurons=30, connect_input=True,\n",
    "                loss_fn='mae', learning_rate=default_params['learning_rate'])\n",
    "params_dist = {\n",
    "    'num_layers': [1, 2, 3, 4],\n",
    "    'num_neurons': np.arange(1, 100, 5),\n",
    "    'connect_input': [True, False],\n",
    "    'loss_fn': ['mse'],\n",
    "    'learning_rate': reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(model_reg, params_dist, n_iter=10, cv=3)        \n",
    "rnd_search_cv.fit(*train_t, epochs=default_params['epochs'], \n",
    "                    validation_data=val_t,    # Used for early stoppage\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:\n",
      "{'connect_input': False, 'learning_rate': 0.0003068775487284945, 'loss_fn': 'mse', 'num_layers': 3, 'num_neurons': 36}\n",
      "Best Score: -2500601.467032258\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Params:\")\n",
    "print(rnd_search_cv.best_params_)\n",
    "print(f\"Best Score: {rnd_search_cv.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = rnd_search_cv.best_estimator_.model\n",
    "tf.keras.utils.plot_model(best_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Comparison Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareName:\n",
    "    def __init__(self, vec_pp, pred_model, glove_df, include_age):\n",
    "        self.vec_pp = vec_pp\n",
    "        self.pred_model = pred_model\n",
    "        self.glove_df = glove_df\n",
    "        self.glove_set = set(glove_df.index)\n",
    "        self.include_age = include_age\n",
    "\n",
    "    def _get_vec_df(self, words: Sequence[str], include_age: bool=False):\n",
    "        # unmappable = {w.lower() for w in words} - self.glove_set\n",
    "        # if len(unmappable) != 0:\n",
    "        #     raise Exception(f'The following words cannot be analyzed: {unmappable}')\n",
    "        df = pd.DataFrame({'name': words}, index=words)\n",
    "        df = self.vec_pp.transform(df)\n",
    "        if include_age:\n",
    "            df['timestamp'] = 0.0\n",
    "        return df\n",
    "    \n",
    "    def get_value(self, words: Sequence[str]):\n",
    "        df = self._get_vec_df(words, self.include_age)\n",
    "        return pd.DataFrame(self.pred_model.predict(df), index=df.index, columns=['est_value'])\n",
    "        \n",
    "    def get_similar(self, words: Sequence[str], limit: int=10):\n",
    "        df = self._get_vec_df(words)\n",
    "        sim_score_df = self.glove_df @ df.T\n",
    "        return sim_score_df.apply(lambda col_ss: col_ss.sort_values(ascending=False)[:limit].index.values)\n",
    "\n",
    "    def get_similar_value(self, words: Sequence[str], limit: int=10):\n",
    "        sim_df = self.get_similar(words, limit=limit)\n",
    "        res_sss = {}\n",
    "        for c in sim_df.columns:\n",
    "            res_sss[c] = self.get_value(sim_df[c].values).sort_values(ascending=False, by='est_value').index\n",
    "        res_df = pd.DataFrame(res_sss)\n",
    "        res_df.index = pd.Index(data=range(1, limit+1), name='rank')\n",
    "        return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>est_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>2185.186279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dog</th>\n",
       "      <td>2041.325562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ape</th>\n",
       "      <td>1901.707153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kittens</th>\n",
       "      <td>1501.318604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mango</th>\n",
       "      <td>1056.713135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Banana</th>\n",
       "      <td>575.959412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           est_value\n",
       "Apple    2185.186279\n",
       "Dog      2041.325562\n",
       "Ape      1901.707153\n",
       "Kittens  1501.318604\n",
       "Mango    1056.713135\n",
       "Banana    575.959412"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    compare = CompareName(nft_vec_pp, best_model, glove_df, default_params['include_nft_age'])\n",
    "    words = ['Apple', 'Mango', 'Banana', 'Kittens', 'Dog', 'Ape']\n",
    "    compare.get_value(words).sort_values('est_value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "      <th>Mango</th>\n",
       "      <th>Banana</th>\n",
       "      <th>Kittens</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Ape</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iphone</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>peanut</td>\n",
       "      <td>cute</td>\n",
       "      <td>dog</td>\n",
       "      <td>homo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ipad</td>\n",
       "      <td>tomato</td>\n",
       "      <td>mango</td>\n",
       "      <td>puppy</td>\n",
       "      <td>dogs</td>\n",
       "      <td>ape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ipod</td>\n",
       "      <td>mango</td>\n",
       "      <td>fruit</td>\n",
       "      <td>puppies</td>\n",
       "      <td>hound</td>\n",
       "      <td>apes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple</td>\n",
       "      <td>apricot</td>\n",
       "      <td>papaya</td>\n",
       "      <td>cat</td>\n",
       "      <td>puppy</td>\n",
       "      <td>tarzan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>microsoft</td>\n",
       "      <td>guava</td>\n",
       "      <td>avocado</td>\n",
       "      <td>kitten</td>\n",
       "      <td>cat</td>\n",
       "      <td>frog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>google</td>\n",
       "      <td>chutney</td>\n",
       "      <td>sugar</td>\n",
       "      <td>kittens</td>\n",
       "      <td>pet</td>\n",
       "      <td>chimpanzee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>macintosh</td>\n",
       "      <td>papaya</td>\n",
       "      <td>coconut</td>\n",
       "      <td>tabby</td>\n",
       "      <td>sled</td>\n",
       "      <td>monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>itunes</td>\n",
       "      <td>avocado</td>\n",
       "      <td>pineapple</td>\n",
       "      <td>puss</td>\n",
       "      <td>horse</td>\n",
       "      <td>hairy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>intel</td>\n",
       "      <td>coconut</td>\n",
       "      <td>banana</td>\n",
       "      <td>gisbergen</td>\n",
       "      <td>animal</td>\n",
       "      <td>creature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ibm</td>\n",
       "      <td>pineapple</td>\n",
       "      <td>bananas</td>\n",
       "      <td>t-ara</td>\n",
       "      <td>terrier</td>\n",
       "      <td>gorilla</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Apple      Mango     Banana    Kittens      Dog         Ape\n",
       "rank                                                                 \n",
       "1        iphone   cucumber     peanut       cute      dog        homo\n",
       "2          ipad     tomato      mango      puppy     dogs         ape\n",
       "3          ipod      mango      fruit    puppies    hound        apes\n",
       "4         apple    apricot     papaya        cat    puppy      tarzan\n",
       "5     microsoft      guava    avocado     kitten      cat        frog\n",
       "6        google    chutney      sugar    kittens      pet  chimpanzee\n",
       "7     macintosh     papaya    coconut      tabby     sled      monkey\n",
       "8        itunes    avocado  pineapple       puss    horse       hairy\n",
       "9         intel    coconut     banana  gisbergen   animal    creature\n",
       "10          ibm  pineapple    bananas      t-ara  terrier     gorilla"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suggestion on which alternative words can be used\n",
    "compare.get_similar_value(words, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9c743f041378de343b56af4b7899ce17768575f4a2c4e3288560c9c11d41700"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
